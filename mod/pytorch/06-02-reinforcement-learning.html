<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aprenentatge per Reforç amb PyTorch</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/06-02-reinforcement-learning" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/06-02-reinforcement-learning" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/06-02-reinforcement-learning" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="https://enterprisecampus.net/mod/pytorch/06-02-reinforcement-learning" class="px-2">EN</a></b>
				|
				<a href="https://campusempresa.com/mod/pytorch/06-02-reinforcement-learning" class="px-2">ES</a></b>
				|
				<b class="px-2">CA</b>
											</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Projecte</a>
				<a href="/about">Sobre nosaltres</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donacions</a>
				<a href="/licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='06-01-gans' title="Xarxes Generatives Adversàries (GANs)">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Aprenentatge per Reforç amb PyTorch</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='06-03-deploying-models' title="Desplegament de Models PyTorch">Següent &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introducció</h1>
<div class='content'><p>L'aprenentatge per reforç (RL) és una branca de l'aprenentatge automàtic on un agent aprèn a prendre decisions mitjançant la interacció amb un entorn. L'objectiu és maximitzar una recompensa acumulada al llarg del temps. En aquest mòdul, explorarem els conceptes bàsics de l'aprenentatge per reforç i com implementar-lo utilitzant PyTorch.</p>
</div><h1>Conceptes Clau</h1>
<div class='content'><ol>
<li><strong>Agent</strong>: L'entitat que pren decisions.</li>
<li><strong>Entorn</strong>: El món amb el qual l'agent interactua.</li>
<li><strong>Acció (A)</strong>: Les decisions que l'agent pot prendre.</li>
<li><strong>Estat (S)</strong>: La representació de la situació actual de l'entorn.</li>
<li><strong>Recompensa (R)</strong>: El feedback que l'agent rep després de prendre una acció.</li>
<li><strong>Política (π)</strong>: La funció que l'agent utilitza per decidir quina acció prendre en cada estat.</li>
<li><strong>Funció de valor (V)</strong>: Una funció que estima la recompensa esperada a llarg termini des d'un estat donat.</li>
</ol>
</div><h1>Implementació d'un Agent de Q-Learning amb PyTorch</h1>
<div class='content'></div><h2>Pas 1: Configuració de l'Entorn</h2>
<div class='content'><p>Utilitzarem <code>gym</code>, una biblioteca popular per a l'aprenentatge per reforç, per crear el nostre entorn.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGd5bQppbXBvcnQgdG9yY2gKaW1wb3J0IHRvcmNoLm5uIGFzIG5uCmltcG9ydCB0b3JjaC5vcHRpbSBhcyBvcHRpbQppbXBvcnQgbnVtcHkgYXMgbnAKCiMgQ3JlZW0gbCdlbnRvcm4KZW52ID0gZ3ltLm1ha2UoJ0NhcnRQb2xlLXYxJyk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Creem l'entorn
env = gym.make('CartPole-v1')</pre></div><div class='content'></div><h2>Pas 2: Definició de la Xarxa Neuronal</h2>
<div class='content'><p>Definirem una xarxa neuronal simple per estimar la funció Q.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgUU5ldHdvcmsobm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmLCBzdGF0ZV9zaXplLCBhY3Rpb25fc2l6ZSk6CiAgICAgICAgc3VwZXIoUU5ldHdvcmssIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcihzdGF0ZV9zaXplLCA2NCkKICAgICAgICBzZWxmLmZjMiA9IG5uLkxpbmVhcig2NCwgNjQpCiAgICAgICAgc2VsZi5mYzMgPSBubi5MaW5lYXIoNjQsIGFjdGlvbl9zaXplKQogICAgCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMSh4KSkKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMih4KSkKICAgICAgICB4ID0gc2VsZi5mYzMoeCkKICAgICAgICByZXR1cm4geAoKc3RhdGVfc2l6ZSA9IGVudi5vYnNlcnZhdGlvbl9zcGFjZS5zaGFwZVswXQphY3Rpb25fc2l6ZSA9IGVudi5hY3Rpb25fc3BhY2UubgpxX25ldHdvcmsgPSBRTmV0d29yayhzdGF0ZV9zaXplLCBhY3Rpb25fc2l6ZSk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

state_size = env.observation_space.shape[0]
action_size = env.action_space.n
q_network = QNetwork(state_size, action_size)</pre></div><div class='content'></div><h2>Pas 3: Definició de la Política d'Exploració</h2>
<div class='content'><p>Utilitzarem una política epsilon-greedy per equilibrar l'exploració i l'explotació.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGVwc2lsb25fZ3JlZWR5X3BvbGljeShzdGF0ZSwgZXBzaWxvbik6CiAgICBpZiBucC5yYW5kb20ucmFuZCgpIDwgZXBzaWxvbjoKICAgICAgICByZXR1cm4gZW52LmFjdGlvbl9zcGFjZS5zYW1wbGUoKSAgIyBFeHBsb3JhY2nDswogICAgZWxzZToKICAgICAgICBzdGF0ZSA9IHRvcmNoLkZsb2F0VGVuc29yKHN0YXRlKS51bnNxdWVlemUoMCkKICAgICAgICBxX3ZhbHVlcyA9IHFfbmV0d29yayhzdGF0ZSkKICAgICAgICByZXR1cm4gdG9yY2guYXJnbWF4KHFfdmFsdWVzKS5pdGVtKCkgICMgRXhwbG90YWNpw7M="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def epsilon_greedy_policy(state, epsilon):
    if np.random.rand() &lt; epsilon:
        return env.action_space.sample()  # Exploraci&oacute;
    else:
        state = torch.FloatTensor(state).unsqueeze(0)
        q_values = q_network(state)
        return torch.argmax(q_values).item()  # Explotaci&oacute;</pre></div><div class='content'></div><h2>Pas 4: Entrenament de l'Agent</h2>
<div class='content'><p>Definirem el bucle d'entrenament per actualitzar la xarxa neuronal.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("b3B0aW1pemVyID0gb3B0aW0uQWRhbShxX25ldHdvcmsucGFyYW1ldGVycygpLCBscj0wLjAwMSkKY3JpdGVyaW9uID0gbm4uTVNFTG9zcygpCgpudW1fZXBpc29kZXMgPSAxMDAwCmdhbW1hID0gMC45OQplcHNpbG9uID0gMS4wCmVwc2lsb25fZGVjYXkgPSAwLjk5NQptaW5fZXBzaWxvbiA9IDAuMDEKCmZvciBlcGlzb2RlIGluIHJhbmdlKG51bV9lcGlzb2Rlcyk6CiAgICBzdGF0ZSA9IGVudi5yZXNldCgpCiAgICB0b3RhbF9yZXdhcmQgPSAwCiAgICAKICAgIGZvciB0IGluIHJhbmdlKDIwMCk6CiAgICAgICAgYWN0aW9uID0gZXBzaWxvbl9ncmVlZHlfcG9saWN5KHN0YXRlLCBlcHNpbG9uKQogICAgICAgIG5leHRfc3RhdGUsIHJld2FyZCwgZG9uZSwgXyA9IGVudi5zdGVwKGFjdGlvbikKICAgICAgICB0b3RhbF9yZXdhcmQgKz0gcmV3YXJkCiAgICAgICAgCiAgICAgICAgIyBDYWxjdWxhIGVsIHZhbG9yIG9iamVjdGl1CiAgICAgICAgbmV4dF9zdGF0ZSA9IHRvcmNoLkZsb2F0VGVuc29yKG5leHRfc3RhdGUpLnVuc3F1ZWV6ZSgwKQogICAgICAgIHRhcmdldCA9IHJld2FyZCArIGdhbW1hICogdG9yY2gubWF4KHFfbmV0d29yayhuZXh0X3N0YXRlKSkuaXRlbSgpICogKDEgLSBkb25lKQogICAgICAgIAogICAgICAgICMgQWN0dWFsaXR6YSBsYSB4YXJ4YSBuZXVyb25hbAogICAgICAgIHN0YXRlID0gdG9yY2guRmxvYXRUZW5zb3Ioc3RhdGUpLnVuc3F1ZWV6ZSgwKQogICAgICAgIHFfdmFsdWVzID0gcV9uZXR3b3JrKHN0YXRlKQogICAgICAgIHRhcmdldF9mID0gcV92YWx1ZXMuY2xvbmUoKQogICAgICAgIHRhcmdldF9mWzBdW2FjdGlvbl0gPSB0YXJnZXQKICAgICAgICAKICAgICAgICBvcHRpbWl6ZXIuemVyb19ncmFkKCkKICAgICAgICBsb3NzID0gY3JpdGVyaW9uKHFfdmFsdWVzLCB0YXJnZXRfZikKICAgICAgICBsb3NzLmJhY2t3YXJkKCkKICAgICAgICBvcHRpbWl6ZXIuc3RlcCgpCiAgICAgICAgCiAgICAgICAgc3RhdGUgPSBuZXh0X3N0YXRlLnNxdWVlemUoMCkubnVtcHkoKQogICAgICAgIAogICAgICAgIGlmIGRvbmU6CiAgICAgICAgICAgIGJyZWFrCiAgICAKICAgIGVwc2lsb24gPSBtYXgobWluX2Vwc2lsb24sIGVwc2lsb24gKiBlcHNpbG9uX2RlY2F5KQogICAgcHJpbnQoZidFcGlzb2RpIHtlcGlzb2RlKzF9L3tudW1fZXBpc29kZXN9LCBSZWNvbXBlbnNhOiB7dG90YWxfcmV3YXJkfSwgRXBzaWxvbjoge2Vwc2lsb246LjJmfScp"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>optimizer = optim.Adam(q_network.parameters(), lr=0.001)
criterion = nn.MSELoss()

num_episodes = 1000
gamma = 0.99
epsilon = 1.0
epsilon_decay = 0.995
min_epsilon = 0.01

for episode in range(num_episodes):
    state = env.reset()
    total_reward = 0
    
    for t in range(200):
        action = epsilon_greedy_policy(state, epsilon)
        next_state, reward, done, _ = env.step(action)
        total_reward += reward
        
        # Calcula el valor objectiu
        next_state = torch.FloatTensor(next_state).unsqueeze(0)
        target = reward + gamma * torch.max(q_network(next_state)).item() * (1 - done)
        
        # Actualitza la xarxa neuronal
        state = torch.FloatTensor(state).unsqueeze(0)
        q_values = q_network(state)
        target_f = q_values.clone()
        target_f[0][action] = target
        
        optimizer.zero_grad()
        loss = criterion(q_values, target_f)
        loss.backward()
        optimizer.step()
        
        state = next_state.squeeze(0).numpy()
        
        if done:
            break
    
    epsilon = max(min_epsilon, epsilon * epsilon_decay)
    print(f'Episodi {episode+1}/{num_episodes}, Recompensa: {total_reward}, Epsilon: {epsilon:.2f}')</pre></div><div class='content'></div><h2>Pas 5: Validació de l'Agent</h2>
<div class='content'><p>Després de l'entrenament, validarem l'agent per veure com de bé ha après a jugar.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("c3RhdGUgPSBlbnYucmVzZXQoKQp0b3RhbF9yZXdhcmQgPSAwCgpmb3IgdCBpbiByYW5nZSgyMDApOgogICAgZW52LnJlbmRlcigpCiAgICBhY3Rpb24gPSBlcHNpbG9uX2dyZWVkeV9wb2xpY3koc3RhdGUsIDAuMDEpICAjIFV0aWxpdHplbSB1biBlcHNpbG9uIG1vbHQgYmFpeCBwZXIgbWF4aW1pdHphciBsJ2V4cGxvdGFjacOzCiAgICBuZXh0X3N0YXRlLCByZXdhcmQsIGRvbmUsIF8gPSBlbnYuc3RlcChhY3Rpb24pCiAgICB0b3RhbF9yZXdhcmQgKz0gcmV3YXJkCiAgICBzdGF0ZSA9IG5leHRfc3RhdGUKICAgIAogICAgaWYgZG9uZToKICAgICAgICBicmVhawoKcHJpbnQoZidSZWNvbXBlbnNhIHRvdGFsIGR1cmFudCBsYSB2YWxpZGFjacOzOiB7dG90YWxfcmV3YXJkfScpCmVudi5jbG9zZSgp"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>state = env.reset()
total_reward = 0

for t in range(200):
    env.render()
    action = epsilon_greedy_policy(state, 0.01)  # Utilitzem un epsilon molt baix per maximitzar l'explotaci&oacute;
    next_state, reward, done, _ = env.step(action)
    total_reward += reward
    state = next_state
    
    if done:
        break

print(f'Recompensa total durant la validaci&oacute;: {total_reward}')
env.close()</pre></div><div class='content'></div><h1>Exercicis Pràctics</h1>
<div class='content'><ol>
<li><strong>Modifica la Xarxa Neuronal</strong>: Prova d'afegir més capes o canviar la mida de les capes existents per veure com afecta el rendiment de l'agent.</li>
<li><strong>Canvia l'Entorn</strong>: Utilitza un entorn diferent de <code>gym</code> i adapta el codi per entrenar l'agent en aquest nou entorn.</li>
<li><strong>Implementa DQN</strong>: Implementa Deep Q-Learning Network (DQN) amb una xarxa de target per millorar l'estabilitat de l'entrenament.</li>
</ol>
</div><h1>Conclusió</h1>
<div class='content'><p>En aquest mòdul, hem après els conceptes bàsics de l'aprenentatge per reforç i hem implementat un agent de Q-Learning utilitzant PyTorch. Hem explorat com definir una xarxa neuronal, una política d'exploració i com entrenar l'agent. Els exercicis pràctics proporcionats us ajudaran a aprofundir en aquests conceptes i a experimentar amb diferents configuracions.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='06-01-gans' title="Xarxes Generatives Adversàries (GANs)">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='06-03-deploying-models' title="Desplegament de Models PyTorch">Següent &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Acceptar</a>
    <a href="/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>

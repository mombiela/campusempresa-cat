<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <title>Funcions d'Activació</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/02-03-activation-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/02-03-activation-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/02-03-activation-functions" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=4" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
			<span>	<a href="https://enterprisecampus.net/mod/pytorch/02-03-activation-functions" class="px-2">EN</a></b>
	|
	<a href="https://campusempresa.com/mod/pytorch/02-03-activation-functions" class="px-2">ES</a></b>
	|
	<b class="px-2">CA</b>
</span>
			<span class="d-none d-md-inline"><br><cite>Tot el coneixement al teu abast</cite></span>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Projecte</a> | 
<a href="/about">Sobre nosaltres</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donacions</a> | 
<a href="/licence">Llicència</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
					 				<a href="/"><i class="bi bi-house-fill"></i> HOME</a>
											</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-02-creating-simple-neural-network' title="Creació d'una Xarxa Neuronal Simple">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Funcions d'Activació</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-04-loss-functions-optimization' title="Funcions de Pèrdua i Optimització">
				<span class="d-none d-md-inline">Següent &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>Les funcions d'activació són components essencials en les xarxes neuronals, ja que introdueixen no-linealitat en el model, permetent que la xarxa aprengui relacions complexes entre les dades d'entrada i sortida. En aquesta secció, explorarem les funcions d'activació més comunes utilitzades en les xarxes neuronals.</p>
</div><h1>Objectius d'Aprenentatge</h1>
<div class='content'><ul>
<li>Comprendre la importància de les funcions d'activació.</li>
<li>Conèixer les funcions d'activació més utilitzades.</li>
<li>Implementar funcions d'activació en PyTorch.</li>
</ul>
</div><h1><ol>
<li>Importància de les Funcions d'Activació</li>
</ol></h1>
<div class='content'><p>Les funcions d'activació transformen la sortida lineal d'una neurona en una sortida no-lineal. Sense aquestes funcions, una xarxa neuronal, independentment del nombre de capes, es comportaria com un model lineal. Les funcions d'activació permeten que la xarxa aprengui patrons complexos i no-lineals.</p>
</div><h1><ol start="2">
<li>Funcions d'Activació Comunes</li>
</ol></h1>
<div class='content'></div><h2>2.1. Sigmoid</h2>
<div class='content'><p>La funció sigmoid és una funció logística que mapeja qualsevol valor real a un rang entre 0 i 1.</p>
<p><strong>Matemàticament:</strong>
\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Sortida entre 0 i 1.</li>
<li>Bona per a la sortida de probabilitats.</li>
<li>Pot patir de gradients que desapareixen.</li>
</ul>
<p><strong>Implementació en PyTorch:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKc2lnbW9pZCA9IG5uLlNpZ21vaWQoKQp4ID0gdG9yY2gudGVuc29yKFstMS4wLCAwLjAsIDEuMF0pCm91dHB1dCA9IHNpZ21vaWQoeCkKcHJpbnQob3V0cHV0KQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

sigmoid = nn.Sigmoid()
x = torch.tensor([-1.0, 0.0, 1.0])
output = sigmoid(x)
print(output)</pre></div><div class='content'></div><h2>2.2. Tanh</h2>
<div class='content'><p>La funció tanh (tangent hiperbòlica) mapeja qualsevol valor real a un rang entre -1 i 1.</p>
<p><strong>Matemàticament:</strong>
\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Sortida entre -1 i 1.</li>
<li>Centrada en zero, el que pot ajudar a la convergència.</li>
<li>També pot patir de gradients que desapareixen.</li>
</ul>
<p><strong>Implementació en PyTorch:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("dGFuaCA9IG5uLlRhbmgoKQpvdXRwdXQgPSB0YW5oKHgpCnByaW50KG91dHB1dCk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>tanh = nn.Tanh()
output = tanh(x)
print(output)</pre></div><div class='content'></div><h2>2.3. ReLU (Rectified Linear Unit)</h2>
<div class='content'><p>La funció ReLU és una de les funcions d'activació més utilitzades en xarxes neuronals profundes.</p>
<p><strong>Matemàticament:</strong>
\[ \text{ReLU}(x) = \max(0, x) \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Simple i eficient.</li>
<li>No pateix de gradients que desapareixen.</li>
<li>Pot patir de neurones mortes (valors que queden a zero).</li>
</ul>
<p><strong>Implementació en PyTorch:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cmVsdSA9IG5uLlJlTFUoKQpvdXRwdXQgPSByZWx1KHgpCnByaW50KG91dHB1dCk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>relu = nn.ReLU()
output = relu(x)
print(output)</pre></div><div class='content'></div><h2>2.4. Leaky ReLU</h2>
<div class='content'><p>La funció Leaky ReLU és una variant de la ReLU que permet un petit gradient quan l'entrada és negativa.</p>
<p><strong>Matemàticament:</strong>
\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{si } x &gt; 0 <br>\alpha x &amp; \text{si } x \leq 0
\end{cases} \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Evita el problema de neurones mortes.</li>
<li>\(\alpha\) és un petit valor positiu (per exemple, 0.01).</li>
</ul>
<p><strong>Implementació en PyTorch:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("bGVha3lfcmVsdSA9IG5uLkxlYWt5UmVMVSgwLjAxKQpvdXRwdXQgPSBsZWFreV9yZWx1KHgpCnByaW50KG91dHB1dCk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>leaky_relu = nn.LeakyReLU(0.01)
output = leaky_relu(x)
print(output)</pre></div><div class='content'></div><h2>2.5. Softmax</h2>
<div class='content'><p>La funció Softmax es sol utilitzar a la capa de sortida per a problemes de classificació multiclasse.</p>
<p><strong>Matemàticament:</strong>
\[ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Converteix les sortides en probabilitats.</li>
<li>La suma de les sortides és 1.</li>
</ul>
<p><strong>Implementació en PyTorch:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("c29mdG1heCA9IG5uLlNvZnRtYXgoZGltPTApCm91dHB1dCA9IHNvZnRtYXgoeCkKcHJpbnQob3V0cHV0KQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>softmax = nn.Softmax(dim=0)
output = softmax(x)
print(output)</pre></div><div class='content'></div><h1><ol start="3">
<li>Implementació en una Xarxa Neuronal</li>
</ol></h1>
<div class='content'><p>Vegem com utilitzar aquestes funcions d'activació en una xarxa neuronal simple amb PyTorch.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gub3B0aW0gYXMgb3B0aW0KCiMgRGVmaW5pbSB1bmEgeGFyeGEgbmV1cm9uYWwgc2ltcGxlCmNsYXNzIFNpbXBsZU5OKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZik6CiAgICAgICAgc3VwZXIoU2ltcGxlTk4sIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcigxMCwgNTApCiAgICAgICAgc2VsZi5yZWx1ID0gbm4uUmVMVSgpCiAgICAgICAgc2VsZi5mYzIgPSBubi5MaW5lYXIoNTAsIDEpCiAgICAgICAgc2VsZi5zaWdtb2lkID0gbm4uU2lnbW9pZCgpCiAgICAKICAgIGRlZiBmb3J3YXJkKHNlbGYsIHgpOgogICAgICAgIHggPSBzZWxmLmZjMSh4KQogICAgICAgIHggPSBzZWxmLnJlbHUoeCkKICAgICAgICB4ID0gc2VsZi5mYzIoeCkKICAgICAgICB4ID0gc2VsZi5zaWdtb2lkKHgpCiAgICAgICAgcmV0dXJuIHgKCiMgQ3JlZW0gdW5hIGluc3TDoG5jaWEgZGUgbGEgeGFyeGEKbW9kZWwgPSBTaW1wbGVOTigpCgojIERlZmluaW0gdW5hIHDDqHJkdWEgaSB1biBvcHRpbWl0emFkb3IKY3JpdGVyaW9uID0gbm4uQkNFTG9zcygpCm9wdGltaXplciA9IG9wdGltLlNHRChtb2RlbC5wYXJhbWV0ZXJzKCksIGxyPTAuMDEpCgojIEV4ZW1wbGUgZGUgZGFkZXMgZCdlbnRyYWRhCmlucHV0cyA9IHRvcmNoLnJhbmRuKDEsIDEwKQpsYWJlbHMgPSB0b3JjaC50ZW5zb3IoWzEuMF0pCgojIEZvcndhcmQgcGFzcwpvdXRwdXRzID0gbW9kZWwoaW5wdXRzKQpsb3NzID0gY3JpdGVyaW9uKG91dHB1dHMsIGxhYmVscykKCiMgQmFja3dhcmQgcGFzcyBpIG9wdGltaXR6YWNpw7MKb3B0aW1pemVyLnplcm9fZ3JhZCgpCmxvc3MuYmFja3dhcmQoKQpvcHRpbWl6ZXIuc3RlcCgpCgpwcmludChmJ1NvcnRpZGE6IHtvdXRwdXRzLml0ZW0oKX0sIFDDqHJkdWE6IHtsb3NzLml0ZW0oKX0nKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim

# Definim una xarxa neuronal simple
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(50, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

# Creem una inst&agrave;ncia de la xarxa
model = SimpleNN()

# Definim una p&egrave;rdua i un optimitzador
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Exemple de dades d'entrada
inputs = torch.randn(1, 10)
labels = torch.tensor([1.0])

# Forward pass
outputs = model(inputs)
loss = criterion(outputs, labels)

# Backward pass i optimitzaci&oacute;
optimizer.zero_grad()
loss.backward()
optimizer.step()

print(f'Sortida: {outputs.item()}, P&egrave;rdua: {loss.item()}')</pre></div><div class='content'></div><h1>Exercicis Pràctics</h1>
<div class='content'><ol>
<li><strong>Implementa una xarxa neuronal amb la funció d'activació Tanh en lloc de ReLU.</strong></li>
<li><strong>Experimenta amb Leaky ReLU canviant el valor de \(\alpha\). Observa com afecta l'entrenament.</strong></li>
<li><strong>Utilitza la funció Softmax en una xarxa per a un problema de classificació multiclasse.</strong></li>
</ol>
</div><h1>Solucions</h1>
<div class='content'></div><h2>Exercici 1: Implementació amb Tanh</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgU2ltcGxlTk5fVGFuaChubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYpOgogICAgICAgIHN1cGVyKFNpbXBsZU5OX1RhbmgsIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcigxMCwgNTApCiAgICAgICAgc2VsZi50YW5oID0gbm4uVGFuaCgpCiAgICAgICAgc2VsZi5mYzIgPSBubi5MaW5lYXIoNTAsIDEpCiAgICAgICAgc2VsZi5zaWdtb2lkID0gbm4uU2lnbW9pZCgpCiAgICAKICAgIGRlZiBmb3J3YXJkKHNlbGYsIHgpOgogICAgICAgIHggPSBzZWxmLmZjMSh4KQogICAgICAgIHggPSBzZWxmLnRhbmgoeCkKICAgICAgICB4ID0gc2VsZi5mYzIoeCkKICAgICAgICB4ID0gc2VsZi5zaWdtb2lkKHgpCiAgICAgICAgcmV0dXJuIHg="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class SimpleNN_Tanh(nn.Module):
    def __init__(self):
        super(SimpleNN_Tanh, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.tanh = nn.Tanh()
        self.fc2 = nn.Linear(50, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.tanh(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x</pre></div><div class='content'></div><h2>Exercici 2: Experimentació amb Leaky ReLU</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgU2ltcGxlTk5fTGVha3lSZUxVKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgYWxwaGE9MC4wMSk6CiAgICAgICAgc3VwZXIoU2ltcGxlTk5fTGVha3lSZUxVLCBzZWxmKS5fX2luaXRfXygpCiAgICAgICAgc2VsZi5mYzEgPSBubi5MaW5lYXIoMTAsIDUwKQogICAgICAgIHNlbGYubGVha3lfcmVsdSA9IG5uLkxlYWt5UmVMVShhbHBoYSkKICAgICAgICBzZWxmLmZjMiA9IG5uLkxpbmVhcig1MCwgMSkKICAgICAgICBzZWxmLnNpZ21vaWQgPSBubi5TaWdtb2lkKCkKICAgIAogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgeCA9IHNlbGYuZmMxKHgpCiAgICAgICAgeCA9IHNlbGYubGVha3lfcmVsdSh4KQogICAgICAgIHggPSBzZWxmLmZjMih4KQogICAgICAgIHggPSBzZWxmLnNpZ21vaWQoeCkKICAgICAgICByZXR1cm4geA=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class SimpleNN_LeakyReLU(nn.Module):
    def __init__(self, alpha=0.01):
        super(SimpleNN_LeakyReLU, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.leaky_relu = nn.LeakyReLU(alpha)
        self.fc2 = nn.Linear(50, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.leaky_relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x</pre></div><div class='content'></div><h2>Exercici 3: Classificació Multiclasse amb Softmax</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgU2ltcGxlTk5fU29mdG1heChubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYpOgogICAgICAgIHN1cGVyKFNpbXBsZU5OX1NvZnRtYXgsIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcigxMCwgNTApCiAgICAgICAgc2VsZi5yZWx1ID0gbm4uUmVMVSgpCiAgICAgICAgc2VsZi5mYzIgPSBubi5MaW5lYXIoNTAsIDMpICAjIFN1cG9zZW0gMyBjbGFzc2VzCiAgICAgICAgc2VsZi5zb2Z0bWF4ID0gbm4uU29mdG1heChkaW09MSkKICAgIAogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgeCA9IHNlbGYuZmMxKHgpCiAgICAgICAgeCA9IHNlbGYucmVsdSh4KQogICAgICAgIHggPSBzZWxmLmZjMih4KQogICAgICAgIHggPSBzZWxmLnNvZnRtYXgoeCkKICAgICAgICByZXR1cm4geA=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class SimpleNN_Softmax(nn.Module):
    def __init__(self):
        super(SimpleNN_Softmax, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(50, 3)  # Suposem 3 classes
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x</pre></div><div class='content'></div><h1>Resum</h1>
<div class='content'><p>En aquesta secció, hem après sobre les funcions d'activació més comunes utilitzades en les xarxes neuronals, incloent Sigmoid, Tanh, ReLU, Leaky ReLU i Softmax. Hem vist com implementar-les en PyTorch i com utilitzar-les en una xarxa neuronal simple. Les funcions d'activació són crucials per introduir no-linealitat en el model i permetre que la xarxa aprengui relacions complexes.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-02-creating-simple-neural-network' title="Creació d'una Xarxa Neuronal Simple">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-04-loss-functions-optimization' title="Funcions de Pèrdua i Optimització">
				<span class="d-none d-md-inline">Següent &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
				<!-- 
	<h1>Publicitat</h1>
	<p>Aquest espai està destinat a publicitat.</p>
	<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
	<p>Gràcies per col·laborar!</p>
	 -->
			









		</div>
	</div>
</div>

<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Projecte</a> | 
<a href="/about">Sobre nosaltres</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donacions</a> | 
<a href="/licence">Llicència</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir cookies per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Acceptar</a>
    <a href="/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>

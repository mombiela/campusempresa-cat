<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <title>Funció d'activació</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/02-02-funcion-de-activacion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/02-02-funcion-de-activacion" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/02-02-activation-function" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=4" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>var DEVELOP = window.location.href.indexOf("localhost")!=-1 && true;</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<a href="https://enterprisecampus.net/mod/deep_learning/02-02-activation-function" class="px-2">EN</a></b>
	|
	<a href="https://campusempresa.com/mod/deep_learning/02-02-funcion-de-activacion" class="px-2">ES</a></b>
	|
	<b class="px-2">CA</b>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>Tot el coneixement al teu abast</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Projecte</a> | 
<a href="/about">Sobre nosaltres</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donacions</a> | 
<a href="/licence">Llicència</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col" id="left_menu">
					<a href="/"  class="nav-link px-3">
		<i class="bi bi-house-fill"></i>
		HOME
	</a>

	<a href="./"  class="nav-link px-3">
		<i class="bi bi-journal-bookmark"></i>
		Contingut del curs
	</a>

			</div>
		</div>
	</div>
</div>

<!--  
<div class="top-bar container-fluid">
	<div class="container-xxl">
		<nav class="navbar navbar-expand-md p-0">
			<div class="container-fluid">
				<button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
				</button>
				
				<div class="collapse navbar-collapse" id="navbarNav">
					<div class="navbar-nav me-auto">
							<a href="/"  class="nav-link px-3">
		<i class="bi bi-house-fill"></i>
		HOME
	</a>

	<a href="./"  class="nav-link px-3">
		<i class="bi bi-journal-bookmark"></i>
		Contingut del curs
	</a>

					</div>
				</div>			
							</div>
		</nav>
	</div>
</div>
 -->				<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-01-perceptron-perceptron-multicapa' title="Perceptró i Perceptró Multicapa">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<h2 style="text-decoration:underline">Funció d'activació</h2>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-03-propagacion-hacia-adelante-atras' title="Propagació cap endavant i cap enrere">
				<span class="d-none d-md-inline">Següent &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1>Introducció</h1>
<div class='content'><p>Les funcions d'activació són components essencials en les xarxes neuronals, ja que introdueixen no-linealitat en el model, permetent que la xarxa aprengui relacions complexes entre les dades d'entrada i sortida. Sense funcions d'activació, una xarxa neuronal seria simplement una combinació lineal de les seves entrades, limitant la seva capacitat per resoldre problemes complexos.</p>
</div><h1>Tipus de Funcions d'Activació</h1>
<div class='content'></div><h2><ol>
<li>Sigmoide</li>
</ol></h2>
<div class='content'><p>La funció sigmoide és una de les funcions d'activació més antigues i es defineix com:</p>
<p>\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Sortida en el rang (0, 1).</li>
<li>Bona per a problemes de classificació binària.</li>
<li>Problemes de gradient desaparegut en xarxes profundes.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKIyBFeGVtcGxlIGQnw7pzCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludChzaWdtb2lkKHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(sigmoid(x))</pre></div><div class='content'></div><h2><ol start="2">
<li>Tanh (Tangens hiperbòlic)</li>
</ol></h2>
<div class='content'><p>La funció tanh és una altra funció d'activació popular, definida com:</p>
<p>\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Sortida en el rang (-1, 1).</li>
<li>Sovint preferida sobre la sigmoide perquè la seva sortida està centrada en zero.</li>
<li>També pateix del problema de gradient desaparegut.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKIyBFeGVtcGxlIGQnw7pzCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludCh0YW5oKHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def tanh(x):
    return np.tanh(x)

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(tanh(x))</pre></div><div class='content'></div><h2><ol start="3">
<li>ReLU (Rectified Linear Unit)</li>
</ol></h2>
<div class='content'><p>La funció ReLU és actualment una de les funcions d'activació més utilitzades:</p>
<p>\[ \text{ReLU}(x) = \max(0, x) \]</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Introduïda per primera vegada en 2011.</li>
<li>No pateix del problema de gradient desaparegut com la sigmoide i tanh.</li>
<li>Pot patir del problema de &quot;neurones mortes&quot; si moltes sortides són zero.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKIyBFeGVtcGxlIGQnw7pzCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludChyZWx1KHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def relu(x):
    return np.maximum(0, x)

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(relu(x))</pre></div><div class='content'></div><h2><ol start="4">
<li>Leaky ReLU</li>
</ol></h2>
<div class='content'><p>La funció Leaky ReLU és una variant de la ReLU que intenta solucionar el problema de les &quot;neurones mortes&quot;:</p>
<p>\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{si } x &gt; 0 <br>\alpha x &amp; \text{si } x \leq 0
\end{cases} \]</p>
<p>On \(\alpha\) és un petit valor positiu (per exemple, 0.01).</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Permet un petit gradient quan l'entrada és negativa.</li>
<li>Redueix el risc de neurones mortes.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCiMgRXhlbXBsZSBkJ8O6cwp4ID0gbnAuYXJyYXkoWy0xLjAsIDAuMCwgMS4wXSkKcHJpbnQobGVha3lfcmVsdSh4KSk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(leaky_relu(x))</pre></div><div class='content'></div><h2><ol start="5">
<li>Softmax</li>
</ol></h2>
<div class='content'><p>La funció softmax es fa servir principalment en la capa de sortida per a problemes de classificació multiclasse:</p>
<p>\[ \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Converteix un vector de valors en una distribució de probabilitats.</li>
<li>La suma de les sortides és 1.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHNvZnRtYXgoeCk6CiAgICBlX3ggPSBucC5leHAoeCAtIG5wLm1heCh4KSkKICAgIHJldHVybiBlX3ggLyBlX3guc3VtKGF4aXM9MCkKCiMgRXhlbXBsZSBkJ8O6cwp4ID0gbnAuYXJyYXkoWzEuMCwgMi4wLCAzLjBdKQpwcmludChzb2Z0bWF4KHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

# Exemple d'&uacute;s
x = np.array([1.0, 2.0, 3.0])
print(softmax(x))</pre></div><div class='content'></div><h1>Exercicis Pràctics</h1>
<div class='content'></div><h2>Exercici 1: Implementació de Funcions d'Activació</h2>
<div class='content'><p>Implementa les funcions d'activació següents en Python: Sigmoide, Tanh, ReLU, Leaky ReLU i Softmax. Prova-les amb un conjunt de dades d'exemple.</p>
</div><h2>Exercici 2: Comparació de Funcions d'Activació</h2>
<div class='content'><p>Crea un gràfic que compari les sortides de les funcions d'activació Sigmoide, Tanh, ReLU i Leaky ReLU per a valors d'entrada en el rang [-10, 10].</p>
<p><strong>Solució:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdAoKeCA9IG5wLmxpbnNwYWNlKC0xMCwgMTAsIDEwMCkKCnBsdC5wbG90KHgsIHNpZ21vaWQoeCksIGxhYmVsPSdTaWdtb2lkJykKcGx0LnBsb3QoeCwgdGFuaCh4KSwgbGFiZWw9J1RhbmgnKQpwbHQucGxvdCh4LCByZWx1KHgpLCBsYWJlbD0nUmVMVScpCnBsdC5wbG90KHgsIGxlYWt5X3JlbHUoeCksIGxhYmVsPSdMZWFreSBSZUxVJykKcGx0LmxlZ2VuZCgpCnBsdC54bGFiZWwoJ0lucHV0JykKcGx0LnlsYWJlbCgnT3V0cHV0JykKcGx0LnRpdGxlKCdDb21wYXJhY2nDsyBkZSBGdW5jaW9ucyBkXCdBY3RpdmFjacOzJykKcGx0LnNob3coKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import matplotlib.pyplot as plt

x = np.linspace(-10, 10, 100)

plt.plot(x, sigmoid(x), label='Sigmoid')
plt.plot(x, tanh(x), label='Tanh')
plt.plot(x, relu(x), label='ReLU')
plt.plot(x, leaky_relu(x), label='Leaky ReLU')
plt.legend()
plt.xlabel('Input')
plt.ylabel('Output')
plt.title('Comparaci&oacute; de Funcions d\'Activaci&oacute;')
plt.show()</pre></div><div class='content'></div><h1>Conclusió</h1>
<div class='content'><p>Les funcions d'activació són crucials per a les xarxes neuronals, ja que introdueixen no-linealitat i permeten que el model aprengui patrons complexos. Cada funció té els seus avantatges i inconvenients, i la selecció de la funció d'activació adequada pot dependre del problema específic que s'està intentant resoldre.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-01-perceptron-perceptron-multicapa' title="Perceptró i Perceptró Multicapa">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-03-propagacion-hacia-adelante-atras' title="Propagació cap endavant i cap enrere">
				<span class="d-none d-md-inline">Següent &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
				<!-- 
	<h1>Publicitat</h1>
	<p>Aquest espai està destinat a publicitat.</p>
	<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
	<p>Gràcies per col·laborar!</p>
	 -->
			
	<div class="container mt-2 d-none d-md-block index">
		<h1>Curs de Deep Learning</h1>
<h2>Mòdul 1: Introducció a Deep Learning</h2>
<ul>
<li><a href="01-01-que-es-deep-learning">Què és Deep Learning?</a></li>
<li><a href="01-02-historia-evolucion-deep-learning">Història i evolució del Deep Learning</a></li>
<li><a href="01-03-aplicaciones-deep-learning">Aplicacions de Deep Learning</a></li>
<li><a href="01-04-conceptos-basicos-redes-neuronales">Conceptes bàsics de xarxes neuronals</a></li>
</ul>
<h2>Mòdul 2: Fonaments de Xarxes Neuronals</h2>
<ul>
<li><a href="02-01-perceptron-perceptron-multicapa">Perceptró i Perceptró Multicapa</a></li>
<li><a href="02-02-funcion-de-activacion">Funció d'activació</a></li>
<li><a href="02-03-propagacion-hacia-adelante-atras">Propagació cap endavant i cap enrere</a></li>
<li><a href="02-04-optimizacion-funcion-de-perdida">Optimització i funció de pèrdua</a></li>
</ul>
<h2>Mòdul 3: Xarxes Neuronals Convolucionals (CNN)</h2>
<ul>
<li><a href="03-01-introduccion-cnn">Introducció a les CNN</a></li>
<li><a href="03-02-capas-convolucionales-pooling">Capes convolutionals i de pooling</a></li>
<li><a href="03-03-arquitecturas-populares-cnn">Arquitectures populars de CNN</a></li>
<li><a href="03-04-aplicaciones-cnn-reconocimiento-imagenes">Aplicacions de CNN en reconeixement d'imatges</a></li>
</ul>
<h2>Mòdul 4: Xarxes Neuronals Recurrentes (RNN)</h2>
<ul>
<li><a href="04-01-introduccion-rnn">Introducció a les RNN</a></li>
<li><a href="04-02-lstm-gru">LSTM i GRU</a></li>
<li><a href="04-03-aplicaciones-rnn-pln">Aplicacions de RNN en processament del llenguatge natural</a></li>
<li><a href="04-04-secuencias-series-temporales">Seqüències i sèries temporals</a></li>
</ul>
<h2>Mòdul 5: Tècniques Avançades en Deep Learning</h2>
<ul>
<li><a href="05-01-redes-generativas-adversariales">Xarxes Generatives Adversarials (GAN)</a></li>
<li><a href="05-02-autoencoders">Autoencoders</a></li>
<li><a href="05-03-transfer-learning">Transfer Learning</a></li>
<li><a href="05-04-regularizacion-tecnicas-mejora">Regularització i tècniques de millora</a></li>
</ul>
<h2>Mòdul 6: Eines i Frameworks</h2>
<ul>
<li><a href="06-01-introduccion-tensorflow">Introducció a TensorFlow</a></li>
<li><a href="06-02-introduccion-pytorch">Introducció a PyTorch</a></li>
<li><a href="06-03-comparacion-frameworks">Comparació de frameworks</a></li>
<li><a href="06-04-entornos-desarrollo-recursos">Entorns de desenvolupament i recursos addicionals</a></li>
</ul>
<h2>Mòdul 7: Projectes Pràctics</h2>
<ul>
<li><a href="07-01-clasificacion-imagenes-cnn">Classificació d'imatges amb CNN</a></li>
<li><a href="07-02-generacion-texto-rnn">Generació de text amb RNN</a></li>
<li><a href="07-03-deteccion-anomalias-autoencoders">Detecció d'anomalies amb Autoencoders</a></li>
<li><a href="07-04-creacion-gan-generacion-imagenes">Creació d'una GAN per generació d'imatges</a></li>
</ul>
<h2>Mòdul 8: Consideracions Ètiques i Futur del Deep Learning</h2>
<ul>
<li><a href="08-01-etica-deep-learning">Ètica en Deep Learning</a></li>
<li><a href="08-02-impacto-social-economico">Impacte social i econòmic</a></li>
<li><a href="08-03-tendencias-futuras-deep-learning">Tendències futures en Deep Learning</a></li>
<li><a href="08-04-desafios-oportunidades">Desafiaments i oportunitats</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Projecte</a> | 
<a href="/about">Sobre nosaltres</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donacions</a> | 
<a href="/licence">Llicència</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir cookies per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Acceptar</a>
    <a href="/cookies">Més informació</a>
</div>	

	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>

<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <title>Funció d'activació</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/02-02-funcion-de-activacion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/02-02-funcion-de-activacion" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/02-02-activation-function" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=3" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
			<span>	<a href="https://enterprisecampus.net/mod/deep_learning/02-02-activation-function" class="px-2">EN</a></b>
	|
	<a href="https://campusempresa.com/mod/deep_learning/02-02-funcion-de-activacion" class="px-2">ES</a></b>
	|
	<b class="px-2">CA</b>
</span>
			<span class="d-none d-md-inline"><br><cite>Construint la societat d'avui i del demà</cite></span>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Projecte</a> | 
<a href="/about">Sobre nosaltres</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donacions</a> | 
<a href="/licence">Llicència</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Llenguatges</a>
				 					<a href="/categ/frameworks">Frameworks</a>
				 					<a href="/categ/tech-tools">Eines</a>
				 					<a href="/categ/foundations">Fonaments</a>
				 					<a href="/categ/soft-skills">Competències</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-01-perceptron-perceptron-multicapa' title="Perceptró i Perceptró Multicapa">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Funció d'activació</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-03-propagacion-hacia-adelante-atras' title="Propagació cap endavant i cap enrere">
				<span class="d-none d-md-inline">Següent &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1><p>Introducció</p>
</h1>
<div class='content'><p>Les funcions d'activació són components essencials en les xarxes neuronals, ja que introdueixen no-linealitat en el model, permetent que la xarxa aprengui relacions complexes entre les dades d'entrada i sortida. Sense funcions d'activació, una xarxa neuronal seria simplement una combinació lineal de les seves entrades, limitant la seva capacitat per resoldre problemes complexos.</p>
</div><h1><p>Tipus de Funcions d'Activació</p>
</h1>
<div class='content'></div><h2><ol>
<li>Sigmoide</li>
</ol>
</h2>
<div class='content'><p>La funció sigmoide és una de les funcions d'activació més antigues i es defineix com:</p>
<p>\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Sortida en el rang (0, 1).</li>
<li>Bona per a problemes de classificació binària.</li>
<li>Problemes de gradient desaparegut en xarxes profundes.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKIyBFeGVtcGxlIGQnw7pzCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludChzaWdtb2lkKHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(sigmoid(x))</pre></div><div class='content'></div><h2><ol start="2">
<li>Tanh (Tangens hiperbòlic)</li>
</ol>
</h2>
<div class='content'><p>La funció tanh és una altra funció d'activació popular, definida com:</p>
<p>\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Sortida en el rang (-1, 1).</li>
<li>Sovint preferida sobre la sigmoide perquè la seva sortida està centrada en zero.</li>
<li>També pateix del problema de gradient desaparegut.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKIyBFeGVtcGxlIGQnw7pzCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludCh0YW5oKHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def tanh(x):
    return np.tanh(x)

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(tanh(x))</pre></div><div class='content'></div><h2><ol start="3">
<li>ReLU (Rectified Linear Unit)</li>
</ol>
</h2>
<div class='content'><p>La funció ReLU és actualment una de les funcions d'activació més utilitzades:</p>
<p>\[ \text{ReLU}(x) = \max(0, x) \]</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Introduïda per primera vegada en 2011.</li>
<li>No pateix del problema de gradient desaparegut com la sigmoide i tanh.</li>
<li>Pot patir del problema de &quot;neurones mortes&quot; si moltes sortides són zero.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKIyBFeGVtcGxlIGQnw7pzCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludChyZWx1KHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def relu(x):
    return np.maximum(0, x)

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(relu(x))</pre></div><div class='content'></div><h2><ol start="4">
<li>Leaky ReLU</li>
</ol>
</h2>
<div class='content'><p>La funció Leaky ReLU és una variant de la ReLU que intenta solucionar el problema de les &quot;neurones mortes&quot;:</p>
<p>\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{si } x &gt; 0 <br>\alpha x &amp; \text{si } x \leq 0
\end{cases} \]</p>
<p>On \(\alpha\) és un petit valor positiu (per exemple, 0.01).</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Permet un petit gradient quan l'entrada és negativa.</li>
<li>Redueix el risc de neurones mortes.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCiMgRXhlbXBsZSBkJ8O6cwp4ID0gbnAuYXJyYXkoWy0xLjAsIDAuMCwgMS4wXSkKcHJpbnQobGVha3lfcmVsdSh4KSk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

# Exemple d'&uacute;s
x = np.array([-1.0, 0.0, 1.0])
print(leaky_relu(x))</pre></div><div class='content'></div><h2><ol start="5">
<li>Softmax</li>
</ol>
</h2>
<div class='content'><p>La funció softmax es fa servir principalment en la capa de sortida per a problemes de classificació multiclasse:</p>
<p>\[ \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<p><strong>Característiques:</strong></p>
<ul>
<li>Converteix un vector de valors en una distribució de probabilitats.</li>
<li>La suma de les sortides és 1.</li>
</ul>
<p><strong>Exemple en Python:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHNvZnRtYXgoeCk6CiAgICBlX3ggPSBucC5leHAoeCAtIG5wLm1heCh4KSkKICAgIHJldHVybiBlX3ggLyBlX3guc3VtKGF4aXM9MCkKCiMgRXhlbXBsZSBkJ8O6cwp4ID0gbnAuYXJyYXkoWzEuMCwgMi4wLCAzLjBdKQpwcmludChzb2Z0bWF4KHgpKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

# Exemple d'&uacute;s
x = np.array([1.0, 2.0, 3.0])
print(softmax(x))</pre></div><div class='content'></div><h1><p>Exercicis Pràctics</p>
</h1>
<div class='content'></div><h2><p>Exercici 1: Implementació de Funcions d'Activació</p>
</h2>
<div class='content'><p>Implementa les funcions d'activació següents en Python: Sigmoide, Tanh, ReLU, Leaky ReLU i Softmax. Prova-les amb un conjunt de dades d'exemple.</p>
</div><h2><p>Exercici 2: Comparació de Funcions d'Activació</p>
</h2>
<div class='content'><p>Crea un gràfic que compari les sortides de les funcions d'activació Sigmoide, Tanh, ReLU i Leaky ReLU per a valors d'entrada en el rang [-10, 10].</p>
<p><strong>Solució:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdAoKeCA9IG5wLmxpbnNwYWNlKC0xMCwgMTAsIDEwMCkKCnBsdC5wbG90KHgsIHNpZ21vaWQoeCksIGxhYmVsPSdTaWdtb2lkJykKcGx0LnBsb3QoeCwgdGFuaCh4KSwgbGFiZWw9J1RhbmgnKQpwbHQucGxvdCh4LCByZWx1KHgpLCBsYWJlbD0nUmVMVScpCnBsdC5wbG90KHgsIGxlYWt5X3JlbHUoeCksIGxhYmVsPSdMZWFreSBSZUxVJykKcGx0LmxlZ2VuZCgpCnBsdC54bGFiZWwoJ0lucHV0JykKcGx0LnlsYWJlbCgnT3V0cHV0JykKcGx0LnRpdGxlKCdDb21wYXJhY2nDsyBkZSBGdW5jaW9ucyBkXCdBY3RpdmFjacOzJykKcGx0LnNob3coKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import matplotlib.pyplot as plt

x = np.linspace(-10, 10, 100)

plt.plot(x, sigmoid(x), label='Sigmoid')
plt.plot(x, tanh(x), label='Tanh')
plt.plot(x, relu(x), label='ReLU')
plt.plot(x, leaky_relu(x), label='Leaky ReLU')
plt.legend()
plt.xlabel('Input')
plt.ylabel('Output')
plt.title('Comparaci&oacute; de Funcions d\'Activaci&oacute;')
plt.show()</pre></div><div class='content'></div><h1><p>Conclusió</p>
</h1>
<div class='content'><p>Les funcions d'activació són crucials per a les xarxes neuronals, ja que introdueixen no-linealitat i permeten que el model aprengui patrons complexos. Cada funció té els seus avantatges i inconvenients, i la selecció de la funció d'activació adequada pot dependre del problema específic que s'està intentant resoldre.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-01-perceptron-perceptron-multicapa' title="Perceptró i Perceptró Multicapa">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-03-propagacion-hacia-adelante-atras' title="Propagació cap endavant i cap enrere">
				<span class="d-none d-md-inline">Següent &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<!-- 
<h1>Publicitat</h1>
<p>Aquest espai està destinat a publicitat.</p>
<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
<p>Gràcies per col·laborar!</p>
-->

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725"
     crossorigin="anonymous"></script>
<!-- enterprise_campus -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-0611338592562725"
     data-ad-slot="6914733106"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</div>
</div>

<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Projecte</a> | 
<a href="/about">Sobre nosaltres</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donacions</a> | 
<a href="/licence">Llicència</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir cookies per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Acceptar</a>
    <a href="/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>

<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow, noarchive">
    <title>Aprenentatge per Reforç</title>

    <link rel="alternate" href="https://campusempresa.com/mod/ia_videojuegos/04-03-aprendizaje-refuerzo" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/ia_videojuegos/04-03-aprenentatge-reforc" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/ia_videojuegos/04-03-reinforcement-learning" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=4" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>var DEVELOP = window.location.href.indexOf("localhost")!=-1 && true;</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<a href="https://enterprisecampus.net/mod/ia_videojuegos/04-03-reinforcement-learning" class="px-2">EN</a></b>
	|
	<a href="https://campusempresa.com/mod/ia_videojuegos/04-03-aprendizaje-refuerzo" class="px-2">ES</a></b>
	|
	<b class="px-2">CA</b>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>Tot el coneixement al teu abast</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Projecte</a> | 
<a href="/about">Sobre nosaltres</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donacions</a> | 
<a href="/licence">Llicència</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col" id="left_menu">
					<a href="/"  class="nav-link px-3">
		<i class="bi bi-house-fill"></i>
		HOME
	</a>

	<a href="./"  class="nav-link px-3">
		<i class="bi bi-journal-bookmark"></i>
		Contingut del curs
	</a>

			</div>
		</div>
	</div>
</div>

<!--  
<div class="top-bar container-fluid">
	<div class="container-xxl">
		<nav class="navbar navbar-expand-md p-0">
			<div class="container-fluid">
				<button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
				</button>
				
				<div class="collapse navbar-collapse" id="navbarNav">
					<div class="navbar-nav me-auto">
							<a href="/"  class="nav-link px-3">
		<i class="bi bi-house-fill"></i>
		HOME
	</a>

	<a href="./"  class="nav-link px-3">
		<i class="bi bi-journal-bookmark"></i>
		Contingut del curs
	</a>

					</div>
				</div>			
							</div>
		</nav>
	</div>
</div>
 -->				<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='04-02-xarxes-neuronals' title="Xarxes Neuronals en Videojocs">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<h2 style="text-decoration:underline">Aprenentatge per Reforç</h2>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-04-implementacio-agent' title="Implementació d'un Agent d'Aprenentatge">
				<span class="d-none d-md-inline">Següent &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1>Introducció</h1>
<div class='content'><p>L'aprenentatge per reforç (Reinforcement Learning, RL) és una branca de l'aprenentatge automàtic que es basa en la idea d'entrenar agents perquè prenguin decisions seqüencials per maximitzar una recompensa acumulada. En el context dels videojocs, l'agent pot ser un personatge no jugador (NPC) que aprèn a jugar o a completar tasques específiques dins del joc.</p>
</div><h1>Conceptes Clau</h1>
<div class='content'><ol>
<li><strong>Agent</strong>: L'entitat que pren decisions en l'entorn.</li>
<li><strong>Entorn</strong>: El món amb el qual interactua l'agent.</li>
<li><strong>Acció</strong>: Una decisió presa per l'agent que afecta l'entorn.</li>
<li><strong>Estat</strong>: Una representació de la situació actual de l'entorn.</li>
<li><strong>Recompensa</strong>: Un valor numèric que l'agent rep després de prendre una acció, indicant el resultat d'aquesta acció.</li>
<li><strong>Política</strong>: Una estratègia que l'agent segueix per decidir quina acció prendre en cada estat.</li>
<li><strong>Funció de valor</strong>: Una funció que estima la recompensa esperada a llarg termini per a cada estat o estat-acció.</li>
</ol>
</div><h1>Funcionament de l'Aprenentatge per Reforç</h1>
<div class='content'><p>L'agent interactua amb l'entorn en una sèrie de passos. En cada pas, l'agent observa l'estat actual de l'entorn, pren una acció basada en la seva política, i rep una recompensa juntament amb el nou estat resultant. L'objectiu de l'agent és aprendre una política que maximitzi la recompensa acumulada a llarg termini.</p>
</div><h1>Algoritmes d'Aprenentatge per Reforç</h1>
<div class='content'></div><h2>Q-Learning</h2>
<div class='content'><p>Q-Learning és un dels algoritmes més coneguts d'aprenentatge per reforç. Utilitza una taula Q per emmagatzemar els valors de les accions en cada estat. La taula Q s'actualitza iterativament utilitzant la següent fórmula:</p>
<p>\[ Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right] \]</p>
<p>On:</p>
<ul>
<li>\( s \) és l'estat actual.</li>
<li>\( a \) és l'acció presa.</li>
<li>\( r \) és la recompensa rebuda.</li>
<li>\( s' \) és el nou estat resultant.</li>
<li>\( \alpha \) és el ritme d'aprenentatge.</li>
<li>\( \gamma \) és el factor de descompte.</li>
</ul>
</div><h2>Deep Q-Learning (DQN)</h2>
<div class='content'><p>Deep Q-Learning utilitza xarxes neuronals per aproximar la funció Q en lloc d'utilitzar una taula. Això permet manejar entorns amb espais d'estats molt grans o continus. La xarxa neuronal pren com a entrada l'estat actual i produeix els valors Q per a totes les accions possibles.</p>
</div><h1>Exemple Pràctic: Implementació de Q-Learning</h1>
<div class='content'><p>A continuació, es presenta un exemple d'implementació de Q-Learning en Python per a un agent que aprèn a moure's en una quadrícula.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCByYW5kb20KCiMgUGFyw6BtZXRyZXMgZGUgbCdlbnRvcm4Kbl9lc3RhdHMgPSA1Cm5fYWNjaW9ucyA9IDIKZ2FtbWEgPSAwLjkKYWxwaGEgPSAwLjEKZXBpc29kaXMgPSAxMDAwCgojIEluaWNpYWxpdHphciBsYSB0YXVsYSBRClEgPSBucC56ZXJvcygobl9lc3RhdHMsIG5fYWNjaW9ucykpCgojIEZ1bmNpw7MgZGUgcmVjb21wZW5zYQpkZWYgcmVjb21wZW5zYShzLCBhKToKICAgIGlmIHMgPT0gNDoKICAgICAgICByZXR1cm4gMQogICAgZWxzZToKICAgICAgICByZXR1cm4gMAoKIyBFbnRyZW5hbWVudCBkZSBsJ2FnZW50CmZvciBlcGlzb2RpIGluIHJhbmdlKGVwaXNvZGlzKToKICAgIHMgPSByYW5kb20ucmFuZGludCgwLCBuX2VzdGF0cyAtIDEpCiAgICB3aGlsZSBzICE9IDQ6CiAgICAgICAgYSA9IHJhbmRvbS5yYW5kaW50KDAsIG5fYWNjaW9ucyAtIDEpCiAgICAgICAgc19wcmltZSA9IChzICsgYSkgJSBuX2VzdGF0cwogICAgICAgIHIgPSByZWNvbXBlbnNhKHMsIGEpCiAgICAgICAgUVtzLCBhXSA9IFFbcywgYV0gKyBhbHBoYSAqIChyICsgZ2FtbWEgKiBucC5tYXgoUVtzX3ByaW1lLCA6XSkgLSBRW3MsIGFdKQogICAgICAgIHMgPSBzX3ByaW1lCgojIFBvbMOtdGljYSByZXN1bHRhbnQKcG9sw610aWNhID0gbnAuYXJnbWF4KFEsIGF4aXM9MSkKcHJpbnQoIlBvbMOtdGljYToiLCBwb2zDrXRpY2Ep"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import random

# Par&agrave;metres de l'entorn
n_estats = 5
n_accions = 2
gamma = 0.9
alpha = 0.1
episodis = 1000

# Inicialitzar la taula Q
Q = np.zeros((n_estats, n_accions))

# Funci&oacute; de recompensa
def recompensa(s, a):
    if s == 4:
        return 1
    else:
        return 0

# Entrenament de l'agent
for episodi in range(episodis):
    s = random.randint(0, n_estats - 1)
    while s != 4:
        a = random.randint(0, n_accions - 1)
        s_prime = (s + a) % n_estats
        r = recompensa(s, a)
        Q[s, a] = Q[s, a] + alpha * (r + gamma * np.max(Q[s_prime, :]) - Q[s, a])
        s = s_prime

# Pol&iacute;tica resultant
pol&iacute;tica = np.argmax(Q, axis=1)
print(&quot;Pol&iacute;tica:&quot;, pol&iacute;tica)</pre></div><div class='content'></div><h1>Exercici Pràctic</h1>
<div class='content'><p><strong>Exercici:</strong> Implementa un agent de Q-Learning per a un entorn de laberint on l'objectiu és trobar la sortida. Defineix l'entorn, les recompenses i les accions possibles. Entrena l'agent i mostra la política resultant.</p>
<p><strong>Solució:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCByYW5kb20KCiMgRGVmaW5pY2nDsyBkZSBsJ2VudG9ybiBkZWwgbGFiZXJpbnQKbGFiZXJpbnQgPSBbCiAgICBbMCwgMCwgMCwgMV0sCiAgICBbMSwgMSwgMCwgMV0sCiAgICBbMCwgMCwgMCwgMF0sCiAgICBbMCwgMSwgMSwgMF0sCiAgICBbMCwgMCwgMCwgMF0KXQoKbl9maWxlcyA9IGxlbihsYWJlcmludCkKbl9jb2x1bW5lcyA9IGxlbihsYWJlcmludFswXSkKbl9hY2Npb25zID0gNCAgIyBBbXVudCwgQXZhbGwsIEVzcXVlcnJhLCBEcmV0YQpnYW1tYSA9IDAuOQphbHBoYSA9IDAuMQplcGlzb2RpcyA9IDEwMDAKCiMgSW5pY2lhbGl0emFyIGxhIHRhdWxhIFEKUSA9IG5wLnplcm9zKChuX2ZpbGVzLCBuX2NvbHVtbmVzLCBuX2FjY2lvbnMpKQoKIyBGdW5jacOzIGRlIHJlY29tcGVuc2EKZGVmIHJlY29tcGVuc2Eocyk6CiAgICBpZiBzID09ICg0LCAzKToKICAgICAgICByZXR1cm4gMQogICAgZWxzZToKICAgICAgICByZXR1cm4gLTAuMQoKIyBGdW5jacOzIHBlciBvYnRlbmlyIGVsIG5vdSBlc3RhdCBkZXNwcsOpcyBkZSBwcmVuZHJlIHVuYSBhY2Npw7MKZGVmIG5vdV9lc3RhdChzLCBhKToKICAgIGlmIGEgPT0gMDogICMgQW11bnQKICAgICAgICByZXR1cm4gKG1heChzWzBdIC0gMSwgMCksIHNbMV0pCiAgICBlbGlmIGEgPT0gMTogICMgQXZhbGwKICAgICAgICByZXR1cm4gKG1pbihzWzBdICsgMSwgbl9maWxlcyAtIDEpLCBzWzFdKQogICAgZWxpZiBhID09IDI6ICAjIEVzcXVlcnJhCiAgICAgICAgcmV0dXJuIChzWzBdLCBtYXgoc1sxXSAtIDEsIDApKQogICAgZWxpZiBhID09IDM6ICAjIERyZXRhCiAgICAgICAgcmV0dXJuIChzWzBdLCBtaW4oc1sxXSArIDEsIG5fY29sdW1uZXMgLSAxKSkKCiMgRW50cmVuYW1lbnQgZGUgbCdhZ2VudApmb3IgZXBpc29kaSBpbiByYW5nZShlcGlzb2Rpcyk6CiAgICBzID0gKDAsIDApCiAgICB3aGlsZSBzICE9ICg0LCAzKToKICAgICAgICBhID0gcmFuZG9tLnJhbmRpbnQoMCwgbl9hY2Npb25zIC0gMSkKICAgICAgICBzX3ByaW1lID0gbm91X2VzdGF0KHMsIGEpCiAgICAgICAgciA9IHJlY29tcGVuc2Eoc19wcmltZSkKICAgICAgICBRW3NbMF0sIHNbMV0sIGFdID0gUVtzWzBdLCBzWzFdLCBhXSArIGFscGhhICogKHIgKyBnYW1tYSAqIG5wLm1heChRW3NfcHJpbWVbMF0sIHNfcHJpbWVbMV0sIDpdKSAtIFFbc1swXSwgc1sxXSwgYV0pCiAgICAgICAgcyA9IHNfcHJpbWUKCiMgUG9sw610aWNhIHJlc3VsdGFudApwb2zDrXRpY2EgPSBucC5hcmdtYXgoUSwgYXhpcz0yKQpwcmludCgiUG9sw610aWNhOiIpCnByaW50KHBvbMOtdGljYSk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import random

# Definici&oacute; de l'entorn del laberint
laberint = [
    [0, 0, 0, 1],
    [1, 1, 0, 1],
    [0, 0, 0, 0],
    [0, 1, 1, 0],
    [0, 0, 0, 0]
]

n_files = len(laberint)
n_columnes = len(laberint[0])
n_accions = 4  # Amunt, Avall, Esquerra, Dreta
gamma = 0.9
alpha = 0.1
episodis = 1000

# Inicialitzar la taula Q
Q = np.zeros((n_files, n_columnes, n_accions))

# Funci&oacute; de recompensa
def recompensa(s):
    if s == (4, 3):
        return 1
    else:
        return -0.1

# Funci&oacute; per obtenir el nou estat despr&eacute;s de prendre una acci&oacute;
def nou_estat(s, a):
    if a == 0:  # Amunt
        return (max(s[0] - 1, 0), s[1])
    elif a == 1:  # Avall
        return (min(s[0] + 1, n_files - 1), s[1])
    elif a == 2:  # Esquerra
        return (s[0], max(s[1] - 1, 0))
    elif a == 3:  # Dreta
        return (s[0], min(s[1] + 1, n_columnes - 1))

# Entrenament de l'agent
for episodi in range(episodis):
    s = (0, 0)
    while s != (4, 3):
        a = random.randint(0, n_accions - 1)
        s_prime = nou_estat(s, a)
        r = recompensa(s_prime)
        Q[s[0], s[1], a] = Q[s[0], s[1], a] + alpha * (r + gamma * np.max(Q[s_prime[0], s_prime[1], :]) - Q[s[0], s[1], a])
        s = s_prime

# Pol&iacute;tica resultant
pol&iacute;tica = np.argmax(Q, axis=2)
print(&quot;Pol&iacute;tica:&quot;)
print(pol&iacute;tica)</pre></div><div class='content'></div><h1>Resum</h1>
<div class='content'><p>En aquesta secció, hem explorat els conceptes bàsics de l'aprenentatge per reforç, incloent-hi els components clau com l'agent, l'entorn, les accions, els estats, les recompenses i les polítiques. Hem vist com funciona l'algoritme de Q-Learning i hem implementat un exemple pràctic. A més, hem proposat un exercici per reforçar els coneixements adquirits. En el proper tema, explorarem com implementar agents d'aprenentatge per reforç més avançats utilitzant xarxes neuronals.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='04-02-xarxes-neuronals' title="Xarxes Neuronals en Videojocs">
				<span class="d-none d-md-inline">&#x25C4; Anterior</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-04-implementacio-agent' title="Implementació d'un Agent d'Aprenentatge">
				<span class="d-none d-md-inline">Següent &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
				<!-- 
	<h1>Publicitat</h1>
	<p>Aquest espai està destinat a publicitat.</p>
	<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
	<p>Gràcies per col·laborar!</p>
	 -->
			
	<div class="container mt-2 d-none d-md-block index">
		<h1>IA per a Videojocs</h1>
<h2>Mòdul 1: Introducció a la IA en Videojocs</h2>
<ul>
<li><a href="01-01-historia-evolucio">Història i Evolució de la IA en Videojocs</a></li>
<li><a href="01-02-conceptes-basics">Conceptes Bàsics de IA</a></li>
<li><a href="01-03-eines-llenguatges">Eines i Llenguatges de Programació</a></li>
</ul>
<h2>Mòdul 2: Navegació en Videojocs</h2>
<ul>
<li><a href="02-01-algoritmes-cerca">Algoritmes de Cerca de Camins</a></li>
<li><a href="02-02-implementacio-a-estrella">Implementació de A*</a></li>
<li><a href="02-03-navegacio-navmesh">Navegació amb NavMesh</a></li>
<li><a href="02-04-evitacio-obstacles">Evitació d'Obstacles</a></li>
</ul>
<h2>Mòdul 3: Presa de Decisions</h2>
<ul>
<li><a href="03-01-maquines-estats-finis">Màquines d'Estats Finits (FSM)</a></li>
<li><a href="03-02-arbres-decisio">Arbres de Decisió</a></li>
<li><a href="03-03-behavior-trees">Behavior Trees</a></li>
<li><a href="03-04-sistemes-regles">Sistemes Basats en Regles</a></li>
</ul>
<h2>Mòdul 4: Aprenentatge Automàtic</h2>
<ul>
<li><a href="04-01-introduccio-aprenentatge">Introducció a l'Aprenentatge Automàtic</a></li>
<li><a href="04-02-xarxes-neuronals">Xarxes Neuronals en Videojocs</a></li>
<li><a href="04-03-aprenentatge-reforc">Aprenentatge per Reforç</a></li>
<li><a href="04-04-implementacio-agent">Implementació d'un Agent d'Aprenentatge</a></li>
</ul>
<h2>Mòdul 5: Integració i Optimització</h2>
<ul>
<li><a href="05-01-integracio-motors">Integració de IA en Motors de Joc</a></li>
<li><a href="05-02-optimitzacio-algoritmes">Optimització d'Algoritmes de IA</a></li>
<li><a href="05-03-proves-depuracio">Proves i Depuració de IA</a></li>
</ul>
<h2>Mòdul 6: Projectes Pràctics</h2>
<ul>
<li><a href="06-01-projecte-navegacio">Projecte 1: Implementació de Navegació Bàsica</a></li>
<li><a href="06-02-projecte-npc">Projecte 2: Creació d'un NPC amb Presa de Decisions</a></li>
<li><a href="06-03-projecte-agent">Projecte 3: Desenvolupament d'un Agent amb Aprenentatge Automàtic</a></li>
</ul>
<h2>Mòdul 7: Recursos Addicionals</h2>
<ul>
<li><a href="07-01-llibres-articles">Llibres i Articles Recomanats</a></li>
<li><a href="07-02-comunitats-forums">Comunitats i Fòrums</a></li>
<li><a href="07-03-eines-llibreries">Eines i Llibreries Útils</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">El Projecte</a> | 
<a href="/about">Sobre nosaltres</a> | 
<a href="/contribute">Contribuir</a> | 
<a href="/donate">Donacions</a> | 
<a href="/licence">Llicència</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir cookies per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Acceptar</a>
    <a href="/cookies">Més informació</a>
</div>	

	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>

<!DOCTYPE html>
<html lang="ca">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Funcions d'activació</title>

    <link rel="alternate" href="https://campusempresa.com/mod/tensorflow/04-03-activation-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/tensorflow/04-03-activation-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/tensorflow/04-03-activation-functions" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Construint la societat d'avui i del demà</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
							<a href="https://enterprisecampus.net/mod/tensorflow/04-03-activation-functions" class="px-2">EN</a></b>
				|
				<a href="https://campusempresa.com/mod/tensorflow/04-03-activation-functions" class="px-2">ES</a></b>
				|
				<b class="px-2">CA</b>
											</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">El Projecte</a>
				<a href="/about">Sobre nosaltres</a>
				<a href="/contribute">Contribuir</a>
				<a href="/donate">Donacions</a>
				<a href="/licence">Llicència</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-creating-a-simple-neural-network' title="Creació d'una xarxa neuronal simple">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Funcions d'activació</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-loss-functions-and-optimizers' title="Funcions de pèrdua i optimitzadors">Següent &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Les funcions d'activació són components essencials en les xarxes neuronals, ja que introdueixen no-linealitat en el model, permetent que la xarxa aprengui relacions complexes entre les dades d'entrada i les sortides. Sense funcions d'activació, una xarxa neuronal seria simplement una combinació lineal de les seves entrades, limitant la seva capacitat per resoldre problemes complexos.</p>
</div><h1>Tipus de Funcions d'Activació</h1>
<div class='content'></div><h2>1. Funció Sigmoide</h2>
<div class='content'><p>La funció sigmoide és una de les funcions d'activació més antigues i utilitzades. Converteix qualsevol valor real en un valor entre 0 i 1.</p>
<p><strong>Equació:</strong>
\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Sortida entre 0 i 1.</li>
<li>Bona per a problemes de classificació binària.</li>
<li>Pot patir el problema del gradient desaparegut.</li>
</ul>
<p><strong>Exemple de codi:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCmRlZiBzaWdtb2lkKHgpOgogICAgcmV0dXJuIDEgLyAoMSArIG5wLmV4cCgteCkpCgp4ID0gbnAubGluc3BhY2UoLTEwLCAxMCwgMTAwKQp5ID0gc2lnbW9pZCh4KQoKcGx0LnBsb3QoeCwgeSkKcGx0LnRpdGxlKCdGdW5jacOzIFNpZ21vaWRlJykKcGx0LnhsYWJlbCgneCcpCnBsdC55bGFiZWwoJ3NpZ21vaWQoeCknKQpwbHQuZ3JpZChUcnVlKQpwbHQuc2hvdygp"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-10, 10, 100)
y = sigmoid(x)

plt.plot(x, y)
plt.title('Funci&oacute; Sigmoide')
plt.xlabel('x')
plt.ylabel('sigmoid(x)')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h2>2. Funció Tanh (Tangens Hiperbòlic)</h2>
<div class='content'><p>La funció tanh és similar a la sigmoide, però la seva sortida està escalada entre -1 i 1.</p>
<p><strong>Equació:</strong>
\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Sortida entre -1 i 1.</li>
<li>Sovint preferida sobre la sigmoide perquè la seva sortida està centrada en zero.</li>
<li>També pot patir el problema del gradient desaparegut.</li>
</ul>
<p><strong>Exemple de codi:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKeSA9IHRhbmgoeCkKCnBsdC5wbG90KHgsIHkpCnBsdC50aXRsZSgnRnVuY2nDsyBUYW5oJykKcGx0LnhsYWJlbCgneCcpCnBsdC55bGFiZWwoJ3RhbmgoeCknKQpwbHQuZ3JpZChUcnVlKQpwbHQuc2hvdygp"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def tanh(x):
    return np.tanh(x)

y = tanh(x)

plt.plot(x, y)
plt.title('Funci&oacute; Tanh')
plt.xlabel('x')
plt.ylabel('tanh(x)')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h2>3. Funció ReLU (Rectified Linear Unit)</h2>
<div class='content'><p>La funció ReLU és actualment una de les funcions d'activació més populars en xarxes neuronals profundes.</p>
<p><strong>Equació:</strong>
\[ \text{ReLU}(x) = \max(0, x) \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Sortida entre 0 i ∞.</li>
<li>Resol el problema del gradient desaparegut.</li>
<li>Pot patir el problema de &quot;neurones mortes&quot; (valors negatius esdevenen zero permanentment).</li>
</ul>
<p><strong>Exemple de codi:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKeSA9IHJlbHUoeCkKCnBsdC5wbG90KHgsIHkpCnBsdC50aXRsZSgnRnVuY2nDsyBSZUxVJykKcGx0LnhsYWJlbCgneCcpCnBsdC55bGFiZWwoJ1JlTFUoeCknKQpwbHQuZ3JpZChUcnVlKQpwbHQuc2hvdygp"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def relu(x):
    return np.maximum(0, x)

y = relu(x)

plt.plot(x, y)
plt.title('Funci&oacute; ReLU')
plt.xlabel('x')
plt.ylabel('ReLU(x)')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h2>4. Funció Leaky ReLU</h2>
<div class='content'><p>La funció Leaky ReLU és una variant de la ReLU que permet un petit gradient per a valors negatius.</p>
<p><strong>Equació:</strong>
\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{si } x &gt; 0 <br>\alpha x &amp; \text{si } x \leq 0
\end{cases} \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Sortida entre -∞ i ∞.</li>
<li>Redueix el problema de &quot;neurones mortes&quot;.</li>
<li>\(\alpha\) és un petit valor positiu (per exemple, 0.01).</li>
</ul>
<p><strong>Exemple de codi:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCnkgPSBsZWFreV9yZWx1KHgpCgpwbHQucGxvdCh4LCB5KQpwbHQudGl0bGUoJ0Z1bmNpw7MgTGVha3kgUmVMVScpCnBsdC54bGFiZWwoJ3gnKQpwbHQueWxhYmVsKCdMZWFreSBSZUxVKHgpJykKcGx0LmdyaWQoVHJ1ZSkKcGx0LnNob3coKQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

y = leaky_relu(x)

plt.plot(x, y)
plt.title('Funci&oacute; Leaky ReLU')
plt.xlabel('x')
plt.ylabel('Leaky ReLU(x)')
plt.grid(True)
plt.show()</pre></div><div class='content'></div><h2>5. Funció Softmax</h2>
<div class='content'><p>La funció Softmax és utilitzada principalment en la capa de sortida per a problemes de classificació multiclasse.</p>
<p><strong>Equació:</strong>
\[ \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<p><strong>Propietats:</strong></p>
<ul>
<li>Converteix un vector de valors en un vector de probabilitats.</li>
<li>La suma de les sortides és 1.</li>
</ul>
<p><strong>Exemple de codi:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHNvZnRtYXgoeCk6CiAgICBlX3ggPSBucC5leHAoeCAtIG5wLm1heCh4KSkKICAgIHJldHVybiBlX3ggLyBlX3guc3VtKGF4aXM9MCkKCnggPSBucC5hcnJheShbMS4wLCAyLjAsIDMuMF0pCnkgPSBzb2Z0bWF4KHgpCgpwcmludCgiU29mdG1heDoiLCB5KQ=="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

x = np.array([1.0, 2.0, 3.0])
y = softmax(x)

print(&quot;Softmax:&quot;, y)</pre></div><div class='content'></div><h1>Exercicis Pràctics</h1>
<div class='content'></div><h2>Exercici 1: Implementar Funcions d'Activació</h2>
<div class='content'><p>Implementa les funcions d'activació Sigmoide, Tanh, ReLU i Leaky ReLU en Python i ploteja les seves gràfiques.</p>
</div><h2>Exercici 2: Aplicar Funcions d'Activació en una Xarxa Neuronal</h2>
<div class='content'><p>Construeix una xarxa neuronal simple utilitzant TensorFlow i aplica diferents funcions d'activació a les capes ocultes. Observa com afecta el rendiment del model.</p>
<p><strong>Solució de l'Exercici 1:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCmRlZiBzaWdtb2lkKHgpOgogICAgcmV0dXJuIDEgLyAoMSArIG5wLmV4cCgteCkpCgpkZWYgdGFuaCh4KToKICAgIHJldHVybiBucC50YW5oKHgpCgpkZWYgcmVsdSh4KToKICAgIHJldHVybiBucC5tYXhpbXVtKDAsIHgpCgpkZWYgbGVha3lfcmVsdSh4LCBhbHBoYT0wLjAxKToKICAgIHJldHVybiBucC53aGVyZSh4ID4gMCwgeCwgYWxwaGEgKiB4KQoKeCA9IG5wLmxpbnNwYWNlKC0xMCwgMTAsIDEwMCkKCnBsdC5maWd1cmUoZmlnc2l6ZT0oMTIsIDgpKQoKcGx0LnN1YnBsb3QoMiwgMiwgMSkKcGx0LnBsb3QoeCwgc2lnbW9pZCh4KSkKcGx0LnRpdGxlKCdGdW5jacOzIFNpZ21vaWRlJykKcGx0LmdyaWQoVHJ1ZSkKCnBsdC5zdWJwbG90KDIsIDIsIDIpCnBsdC5wbG90KHgsIHRhbmgoeCkpCnBsdC50aXRsZSgnRnVuY2nDsyBUYW5oJykKcGx0LmdyaWQoVHJ1ZSkKCnBsdC5zdWJwbG90KDIsIDIsIDMpCnBsdC5wbG90KHgsIHJlbHUoeCkpCnBsdC50aXRsZSgnRnVuY2nDsyBSZUxVJykKcGx0LmdyaWQoVHJ1ZSkKCnBsdC5zdWJwbG90KDIsIDIsIDQpCnBsdC5wbG90KHgsIGxlYWt5X3JlbHUoeCkpCnBsdC50aXRsZSgnRnVuY2nDsyBMZWFreSBSZUxVJykKcGx0LmdyaWQoVHJ1ZSkKCnBsdC50aWdodF9sYXlvdXQoKQpwbHQuc2hvdygp"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

x = np.linspace(-10, 10, 100)

plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.plot(x, sigmoid(x))
plt.title('Funci&oacute; Sigmoide')
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(x, tanh(x))
plt.title('Funci&oacute; Tanh')
plt.grid(True)

plt.subplot(2, 2, 3)
plt.plot(x, relu(x))
plt.title('Funci&oacute; ReLU')
plt.grid(True)

plt.subplot(2, 2, 4)
plt.plot(x, leaky_relu(x))
plt.title('Funci&oacute; Leaky ReLU')
plt.grid(True)

plt.tight_layout()
plt.show()</pre></div><div class='content'><p><strong>Solució de l'Exercici 2:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLm1vZGVscyBpbXBvcnQgU2VxdWVudGlhbApmcm9tIHRlbnNvcmZsb3cua2VyYXMubGF5ZXJzIGltcG9ydCBEZW5zZQoKIyBHZW5lcmFyIGRhZGVzIHNpbnTDqHRpcXVlcwp4X3RyYWluID0gbnAucmFuZG9tLnJhbmQoMTAwMCwgMjApCnlfdHJhaW4gPSBucC5yYW5kb20ucmFuZGludCgyLCBzaXplPSgxMDAwLCAxKSkKCiMgQ29uc3RydWlyIGVsIG1vZGVsCm1vZGVsID0gU2VxdWVudGlhbChbCiAgICBEZW5zZSg2NCwgaW5wdXRfZGltPTIwLCBhY3RpdmF0aW9uPSdyZWx1JyksCiAgICBEZW5zZSg2NCwgYWN0aXZhdGlvbj0ncmVsdScpLAogICAgRGVuc2UoMSwgYWN0aXZhdGlvbj0nc2lnbW9pZCcpCl0pCgojIENvbXBpbGFyIGVsIG1vZGVsCm1vZGVsLmNvbXBpbGUob3B0aW1pemVyPSdhZGFtJywgbG9zcz0nYmluYXJ5X2Nyb3NzZW50cm9weScsIG1ldHJpY3M9WydhY2N1cmFjeSddKQoKIyBFbnRyZW5hciBlbCBtb2RlbAptb2RlbC5maXQoeF90cmFpbiwgeV90cmFpbiwgZXBvY2hzPTEwLCBiYXRjaF9zaXplPTMyKQoKIyBBdmFsdWFyIGVsIG1vZGVsCmxvc3MsIGFjY3VyYWN5ID0gbW9kZWwuZXZhbHVhdGUoeF90cmFpbiwgeV90cmFpbikKcHJpbnQoZidQw6hyZHVhOiB7bG9zc30sIFByZWNpc2nDszoge2FjY3VyYWN5fScp"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Generar dades sint&egrave;tiques
x_train = np.random.rand(1000, 20)
y_train = np.random.randint(2, size=(1000, 1))

# Construir el model
model = Sequential([
    Dense(64, input_dim=20, activation='relu'),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compilar el model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Entrenar el model
model.fit(x_train, y_train, epochs=10, batch_size=32)

# Avaluar el model
loss, accuracy = model.evaluate(x_train, y_train)
print(f'P&egrave;rdua: {loss}, Precisi&oacute;: {accuracy}')</pre></div><div class='content'></div><h1>Conclusió</h1>
<div class='content'><p>Les funcions d'activació són crucials per al rendiment de les xarxes neuronals, ja que introdueixen no-linealitat i permeten que el model aprengui patrons complexos. Hem explorat diverses funcions d'activació, incloent Sigmoide, Tanh, ReLU, Leaky ReLU i Softmax, i hem vist com implementar-les i utilitzar-les en models de TensorFlow. En el proper tema, ens endinsarem en les funcions de pèrdua i els optimitzadors, components essencials per entrenar xarxes neuronals de manera efectiva.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-creating-a-simple-neural-network' title="Creació d'una xarxa neuronal simple">&#x25C4;Anterior</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-loss-functions-and-optimizers' title="Funcions de pèrdua i optimitzadors">Següent &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Publicitat</h1>
			<p>Aquest espai està destinat a publicitat.</p>
			<p>Si vols ser patrocinador, contacta amb nosaltres per incloure enllaços en aquesta zona: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Gràcies per col·laborar!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Acceptar</a>
    <a href="/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>

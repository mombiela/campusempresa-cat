<!DOCTYPE html>
<html lang="ca">
<head>
    <title> Aprenentatge per Reforç </title>
        
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow, noarchive">
    
    <link rel="alternate" href="https://campusempresa.com/cursos/ia-para-videojuegos/04-03-aprendizaje-refuerzo" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/cursos/ia-per-a-videojocs/04-03-aprenentatge-reforc" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/courses/ai-for-video-games/04-03-reinforcement-learning" hreflang="en" />
    
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.1ab297bfa4.css" rel="stylesheet">
	 
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>
  		var LANG = "ca";
  		var CATEGORY = "foundations";
  		var MOD_NAME = "ia_videojuegos";
  		var TEMA_NAME = "4-3";
  		var TYPE = "mod";
  		var PATH = "mod/ia_videojuegos/04-03-aprenentatge-reforc";
  		var IS_INDEX = false;
  	</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="module" src="/js/app.ae32789132.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>
	  	
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-VVPMPJSR3P"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());	
	  gtag('config', 'G-VVPMPJSR3P');
	</script>
</head>

<body class="d-none">
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<a href="/"><img src="/img/logo_header.png" alt="Logo Campus Empresa"></a>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<a href="https://enterprisecampus.net/courses/ai-for-video-games/04-03-reinforcement-learning" class="px-2">EN</a></b>
	|
	<a href="https://campusempresa.com/cursos/ia-para-videojuegos/04-03-aprendizaje-refuerzo" class="px-2">ES</a></b>
	|
	<b class="px-2">CA</b>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>Tot el coneixement al teu abast</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objectiu" rel="nofollow">El Projecte</a> | 
<a href="/sobre-nosaltres" rel="nofollow">Sobre nosaltres</a> | 
<a href="/contribuir" rel="nofollow">Contribuir</a> | 
<a href="/donar" rel="nofollow">Donacions</a> | 
<a href="/llicencia" rel="nofollow">Llicència</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col">
				<div class="d-flex justify-content-between">
					<div class="left">
						<a href="/" class="nav-link px-3" id="btnHome">
	<i class="bi bi-house-fill"></i>
	HOME
</a>

<a href="/els-meus-cursos" class="nav-link px-3 d-none" id="btnMyCourses">
	<i class="bi bi-rocket-takeoff-fill"></i>
	<i><b>Els meus cursos</b></i>
</a>
<a href="/cursos-finalitzats" class="nav-link px-3 d-none" id="trophy_button">
	<i class="bi bi-trophy-fill"></i>
	Finalitzats             
</a>

					</div>
                    <div class="ms-auto right">
                        <a id="user_button" href="#" class="nav-link px-3" data-bs-toggle="modal" data-bs-target="#loginModal">
                            <i id="user_icon" class="bi"></i>                            
                        </a>
                    </div>					
				</div>
			</div>
		</div>
	</div>
</div>

		<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
										<div class="row py-1 m-0" id="buttonsModSection">
	<div class="col-6 p-0" data-mod="ia_videojuegos">
		<a  href="#" class="text-secondary d-none" data-read-mod="ia_videojuegos" data-read-unit="4-3" style="text-decoration:none;">
			<i class="bi bi-check-circle-fill"></i> 
			Marcar com a llegit
		</a>
		<a href="#" class="text-secondary d-none" data-unread-mod="ia_videojuegos" data-unread-unit="4-3" style="text-decoration:none;">
			<i class="bi bi-x-circle-fill"></i>
			Marcar com a no llegit
		</a>
	</div>
	<div class="col-6 text-end p-0">
					<a href="./"  class="nav-link">
				<i class="bi bi-journal-text"></i>
				Contingut del curs
			</a>
			</div>
</div>						<div id="inner_content">
				<div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='04-02-xarxes-neuronals' title="Xarxes Neuronals en Videojocs" class="py-2 px-3 btn btn-primary">
				&#x25C4; Anterior 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='04-02-xarxes-neuronals' title="Xarxes Neuronals en Videojocs" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
					<h1 style="text-decoration:underline">Aprenentatge per Reforç</h1>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='04-04-implementacio-agent' title="Implementació d'un Agent d'Aprenentatge" class="py-2 px-3 btn btn-primary"
				data-read-mod="ia_videojuegos" data-read-unit="4-3">
				Següent &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='04-04-implementacio-agent' title="Implementació d'un Agent d'Aprenentatge" class="py-2 px-3 btn btn-primary" 
				data-read-mod="ia_videojuegos" data-read-unit="4-3">
				 &#x25BA;
			</a>
			</div>
</div>
<div class='content'></div><h2>Introducció</h2>
<div class='content'><p>L'aprenentatge per reforç (Reinforcement Learning, RL) és una branca de l'aprenentatge automàtic que es basa en la idea d'entrenar agents perquè prenguin decisions seqüencials per maximitzar una recompensa acumulada. En el context dels videojocs, l'agent pot ser un personatge no jugador (NPC) que aprèn a jugar o a completar tasques específiques dins del joc.</p>
</div><h2>Conceptes Clau</h2>
<div class='content'><ol>
<li><strong>Agent</strong>: L'entitat que pren decisions en l'entorn.</li>
<li><strong>Entorn</strong>: El món amb el qual interactua l'agent.</li>
<li><strong>Acció</strong>: Una decisió presa per l'agent que afecta l'entorn.</li>
<li><strong>Estat</strong>: Una representació de la situació actual de l'entorn.</li>
<li><strong>Recompensa</strong>: Un valor numèric que l'agent rep després de prendre una acció, indicant el resultat d'aquesta acció.</li>
<li><strong>Política</strong>: Una estratègia que l'agent segueix per decidir quina acció prendre en cada estat.</li>
<li><strong>Funció de valor</strong>: Una funció que estima la recompensa esperada a llarg termini per a cada estat o estat-acció.</li>
</ol>
</div><h2>Funcionament de l'Aprenentatge per Reforç</h2>
<div class='content'><p>L'agent interactua amb l'entorn en una sèrie de passos. En cada pas, l'agent observa l'estat actual de l'entorn, pren una acció basada en la seva política, i rep una recompensa juntament amb el nou estat resultant. L'objectiu de l'agent és aprendre una política que maximitzi la recompensa acumulada a llarg termini.</p>
</div><h2>Algoritmes d'Aprenentatge per Reforç</h2>
<div class='content'></div><h3>Q-Learning</h3>
<div class='content'><p>Q-Learning és un dels algoritmes més coneguts d'aprenentatge per reforç. Utilitza una taula Q per emmagatzemar els valors de les accions en cada estat. La taula Q s'actualitza iterativament utilitzant la següent fórmula:</p>
<p>\[ Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right] \]</p>
<p>On:</p>
<ul>
<li>\( s \) és l'estat actual.</li>
<li>\( a \) és l'acció presa.</li>
<li>\( r \) és la recompensa rebuda.</li>
<li>\( s' \) és el nou estat resultant.</li>
<li>\( \alpha \) és el ritme d'aprenentatge.</li>
<li>\( \gamma \) és el factor de descompte.</li>
</ul>
</div><h3>Deep Q-Learning (DQN)</h3>
<div class='content'><p>Deep Q-Learning utilitza xarxes neuronals per aproximar la funció Q en lloc d'utilitzar una taula. Això permet manejar entorns amb espais d'estats molt grans o continus. La xarxa neuronal pren com a entrada l'estat actual i produeix els valors Q per a totes les accions possibles.</p>
</div><h2>Exemple Pràctic: Implementació de Q-Learning</h2>
<div class='content'><p>A continuació, es presenta un exemple d'implementació de Q-Learning en Python per a un agent que aprèn a moure's en una quadrícula.</p>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCByYW5kb20KCiMgUGFyw6BtZXRyZXMgZGUgbCdlbnRvcm4Kbl9lc3RhdHMgPSA1Cm5fYWNjaW9ucyA9IDIKZ2FtbWEgPSAwLjkKYWxwaGEgPSAwLjEKZXBpc29kaXMgPSAxMDAwCgojIEluaWNpYWxpdHphciBsYSB0YXVsYSBRClEgPSBucC56ZXJvcygobl9lc3RhdHMsIG5fYWNjaW9ucykpCgojIEZ1bmNpw7MgZGUgcmVjb21wZW5zYQpkZWYgcmVjb21wZW5zYShzLCBhKToKICAgIGlmIHMgPT0gNDoKICAgICAgICByZXR1cm4gMQogICAgZWxzZToKICAgICAgICByZXR1cm4gMAoKIyBFbnRyZW5hbWVudCBkZSBsJ2FnZW50CmZvciBlcGlzb2RpIGluIHJhbmdlKGVwaXNvZGlzKToKICAgIHMgPSByYW5kb20ucmFuZGludCgwLCBuX2VzdGF0cyAtIDEpCiAgICB3aGlsZSBzICE9IDQ6CiAgICAgICAgYSA9IHJhbmRvbS5yYW5kaW50KDAsIG5fYWNjaW9ucyAtIDEpCiAgICAgICAgc19wcmltZSA9IChzICsgYSkgJSBuX2VzdGF0cwogICAgICAgIHIgPSByZWNvbXBlbnNhKHMsIGEpCiAgICAgICAgUVtzLCBhXSA9IFFbcywgYV0gKyBhbHBoYSAqIChyICsgZ2FtbWEgKiBucC5tYXgoUVtzX3ByaW1lLCA6XSkgLSBRW3MsIGFdKQogICAgICAgIHMgPSBzX3ByaW1lCgojIFBvbMOtdGljYSByZXN1bHRhbnQKcG9sw610aWNhID0gbnAuYXJnbWF4KFEsIGF4aXM9MSkKcHJpbnQoIlBvbMOtdGljYToiLCBwb2zDrXRpY2Ep"))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import random

# Par&agrave;metres de l'entorn
n_estats = 5
n_accions = 2
gamma = 0.9
alpha = 0.1
episodis = 1000

# Inicialitzar la taula Q
Q = np.zeros((n_estats, n_accions))

# Funci&oacute; de recompensa
def recompensa(s, a):
    if s == 4:
        return 1
    else:
        return 0

# Entrenament de l'agent
for episodi in range(episodis):
    s = random.randint(0, n_estats - 1)
    while s != 4:
        a = random.randint(0, n_accions - 1)
        s_prime = (s + a) % n_estats
        r = recompensa(s, a)
        Q[s, a] = Q[s, a] + alpha * (r + gamma * np.max(Q[s_prime, :]) - Q[s, a])
        s = s_prime

# Pol&iacute;tica resultant
pol&iacute;tica = np.argmax(Q, axis=1)
print(&quot;Pol&iacute;tica:&quot;, pol&iacute;tica)</pre></div><div class='content'></div><h2>Exercici Pràctic</h2>
<div class='content'><p><strong>Exercici:</strong> Implementa un agent de Q-Learning per a un entorn de laberint on l'objectiu és trobar la sortida. Defineix l'entorn, les recompenses i les accions possibles. Entrena l'agent i mostra la política resultant.</p>
<p><strong>Solució:</strong></p>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCByYW5kb20KCiMgRGVmaW5pY2nDsyBkZSBsJ2VudG9ybiBkZWwgbGFiZXJpbnQKbGFiZXJpbnQgPSBbCiAgICBbMCwgMCwgMCwgMV0sCiAgICBbMSwgMSwgMCwgMV0sCiAgICBbMCwgMCwgMCwgMF0sCiAgICBbMCwgMSwgMSwgMF0sCiAgICBbMCwgMCwgMCwgMF0KXQoKbl9maWxlcyA9IGxlbihsYWJlcmludCkKbl9jb2x1bW5lcyA9IGxlbihsYWJlcmludFswXSkKbl9hY2Npb25zID0gNCAgIyBBbXVudCwgQXZhbGwsIEVzcXVlcnJhLCBEcmV0YQpnYW1tYSA9IDAuOQphbHBoYSA9IDAuMQplcGlzb2RpcyA9IDEwMDAKCiMgSW5pY2lhbGl0emFyIGxhIHRhdWxhIFEKUSA9IG5wLnplcm9zKChuX2ZpbGVzLCBuX2NvbHVtbmVzLCBuX2FjY2lvbnMpKQoKIyBGdW5jacOzIGRlIHJlY29tcGVuc2EKZGVmIHJlY29tcGVuc2Eocyk6CiAgICBpZiBzID09ICg0LCAzKToKICAgICAgICByZXR1cm4gMQogICAgZWxzZToKICAgICAgICByZXR1cm4gLTAuMQoKIyBGdW5jacOzIHBlciBvYnRlbmlyIGVsIG5vdSBlc3RhdCBkZXNwcsOpcyBkZSBwcmVuZHJlIHVuYSBhY2Npw7MKZGVmIG5vdV9lc3RhdChzLCBhKToKICAgIGlmIGEgPT0gMDogICMgQW11bnQKICAgICAgICByZXR1cm4gKG1heChzWzBdIC0gMSwgMCksIHNbMV0pCiAgICBlbGlmIGEgPT0gMTogICMgQXZhbGwKICAgICAgICByZXR1cm4gKG1pbihzWzBdICsgMSwgbl9maWxlcyAtIDEpLCBzWzFdKQogICAgZWxpZiBhID09IDI6ICAjIEVzcXVlcnJhCiAgICAgICAgcmV0dXJuIChzWzBdLCBtYXgoc1sxXSAtIDEsIDApKQogICAgZWxpZiBhID09IDM6ICAjIERyZXRhCiAgICAgICAgcmV0dXJuIChzWzBdLCBtaW4oc1sxXSArIDEsIG5fY29sdW1uZXMgLSAxKSkKCiMgRW50cmVuYW1lbnQgZGUgbCdhZ2VudApmb3IgZXBpc29kaSBpbiByYW5nZShlcGlzb2Rpcyk6CiAgICBzID0gKDAsIDApCiAgICB3aGlsZSBzICE9ICg0LCAzKToKICAgICAgICBhID0gcmFuZG9tLnJhbmRpbnQoMCwgbl9hY2Npb25zIC0gMSkKICAgICAgICBzX3ByaW1lID0gbm91X2VzdGF0KHMsIGEpCiAgICAgICAgciA9IHJlY29tcGVuc2Eoc19wcmltZSkKICAgICAgICBRW3NbMF0sIHNbMV0sIGFdID0gUVtzWzBdLCBzWzFdLCBhXSArIGFscGhhICogKHIgKyBnYW1tYSAqIG5wLm1heChRW3NfcHJpbWVbMF0sIHNfcHJpbWVbMV0sIDpdKSAtIFFbc1swXSwgc1sxXSwgYV0pCiAgICAgICAgcyA9IHNfcHJpbWUKCiMgUG9sw610aWNhIHJlc3VsdGFudApwb2zDrXRpY2EgPSBucC5hcmdtYXgoUSwgYXhpcz0yKQpwcmludCgiUG9sw610aWNhOiIpCnByaW50KHBvbMOtdGljYSk="))));alert("Copiat!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import random

# Definici&oacute; de l'entorn del laberint
laberint = [
    [0, 0, 0, 1],
    [1, 1, 0, 1],
    [0, 0, 0, 0],
    [0, 1, 1, 0],
    [0, 0, 0, 0]
]

n_files = len(laberint)
n_columnes = len(laberint[0])
n_accions = 4  # Amunt, Avall, Esquerra, Dreta
gamma = 0.9
alpha = 0.1
episodis = 1000

# Inicialitzar la taula Q
Q = np.zeros((n_files, n_columnes, n_accions))

# Funci&oacute; de recompensa
def recompensa(s):
    if s == (4, 3):
        return 1
    else:
        return -0.1

# Funci&oacute; per obtenir el nou estat despr&eacute;s de prendre una acci&oacute;
def nou_estat(s, a):
    if a == 0:  # Amunt
        return (max(s[0] - 1, 0), s[1])
    elif a == 1:  # Avall
        return (min(s[0] + 1, n_files - 1), s[1])
    elif a == 2:  # Esquerra
        return (s[0], max(s[1] - 1, 0))
    elif a == 3:  # Dreta
        return (s[0], min(s[1] + 1, n_columnes - 1))

# Entrenament de l'agent
for episodi in range(episodis):
    s = (0, 0)
    while s != (4, 3):
        a = random.randint(0, n_accions - 1)
        s_prime = nou_estat(s, a)
        r = recompensa(s_prime)
        Q[s[0], s[1], a] = Q[s[0], s[1], a] + alpha * (r + gamma * np.max(Q[s_prime[0], s_prime[1], :]) - Q[s[0], s[1], a])
        s = s_prime

# Pol&iacute;tica resultant
pol&iacute;tica = np.argmax(Q, axis=2)
print(&quot;Pol&iacute;tica:&quot;)
print(pol&iacute;tica)</pre></div><div class='content'></div><h2>Resum</h2>
<div class='content'><p>En aquesta secció, hem explorat els conceptes bàsics de l'aprenentatge per reforç, incloent-hi els components clau com l'agent, l'entorn, les accions, els estats, les recompenses i les polítiques. Hem vist com funciona l'algoritme de Q-Learning i hem implementat un exemple pràctic. A més, hem proposat un exercici per reforçar els coneixements adquirits. En el proper tema, explorarem com implementar agents d'aprenentatge per reforç més avançats utilitzant xarxes neuronals.</p>
</div><div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='04-02-xarxes-neuronals' title="Xarxes Neuronals en Videojocs" class="py-2 px-3 btn btn-primary">
				&#x25C4; Anterior 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='04-02-xarxes-neuronals' title="Xarxes Neuronals en Videojocs" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='04-04-implementacio-agent' title="Implementació d'un Agent d'Aprenentatge" class="py-2 px-3 btn btn-primary"
				data-read-mod="ia_videojuegos" data-read-unit="4-3">
				Següent &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='04-04-implementacio-agent' title="Implementació d'un Agent d'Aprenentatge" class="py-2 px-3 btn btn-primary" 
				data-read-mod="ia_videojuegos" data-read-unit="4-3">
				 &#x25BA;
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
						
	<div class="container mt-2 d-none d-md-block index">
		<h1>IA per a Videojocs</h1>
<h2>Mòdul 1: Introducció a la IA en Videojocs</h2>
<ul>
<li><a href="01-01-historia-evolucio">Història i Evolució de la IA en Videojocs</a></li>
<li><a href="01-02-conceptes-basics">Conceptes Bàsics de IA</a></li>
<li><a href="01-03-eines-llenguatges">Eines i Llenguatges de Programació</a></li>
</ul>
<h2>Mòdul 2: Navegació en Videojocs</h2>
<ul>
<li><a href="02-01-algoritmes-cerca">Algoritmes de Cerca de Camins</a></li>
<li><a href="02-02-implementacio-a-estrella">Implementació de A*</a></li>
<li><a href="02-03-navegacio-navmesh">Navegació amb NavMesh</a></li>
<li><a href="02-04-evitacio-obstacles">Evitació d'Obstacles</a></li>
</ul>
<h2>Mòdul 3: Presa de Decisions</h2>
<ul>
<li><a href="03-01-maquines-estats-finis">Màquines d'Estats Finits (FSM)</a></li>
<li><a href="03-02-arbres-decisio">Arbres de Decisió</a></li>
<li><a href="03-03-behavior-trees">Behavior Trees</a></li>
<li><a href="03-04-sistemes-regles">Sistemes Basats en Regles</a></li>
</ul>
<h2>Mòdul 4: Aprenentatge Automàtic</h2>
<ul>
<li><a href="04-01-introduccio-aprenentatge">Introducció a l'Aprenentatge Automàtic</a></li>
<li><a href="04-02-xarxes-neuronals">Xarxes Neuronals en Videojocs</a></li>
<li><a href="04-03-aprenentatge-reforc">Aprenentatge per Reforç</a></li>
<li><a href="04-04-implementacio-agent">Implementació d'un Agent d'Aprenentatge</a></li>
</ul>
<h2>Mòdul 5: Integració i Optimització</h2>
<ul>
<li><a href="05-01-integracio-motors">Integració de IA en Motors de Joc</a></li>
<li><a href="05-02-optimitzacio-algoritmes">Optimització d'Algoritmes de IA</a></li>
<li><a href="05-03-proves-depuracio">Proves i Depuració de IA</a></li>
</ul>
<h2>Mòdul 6: Projectes Pràctics</h2>
<ul>
<li><a href="06-01-projecte-navegacio">Projecte 1: Implementació de Navegació Bàsica</a></li>
<li><a href="06-02-projecte-npc">Projecte 2: Creació d'un NPC amb Presa de Decisions</a></li>
<li><a href="06-03-projecte-agent">Projecte 3: Desenvolupament d'un Agent amb Aprenentatge Automàtic</a></li>
</ul>
<h2>Mòdul 7: Recursos Addicionals</h2>
<ul>
<li><a href="07-01-llibres-articles">Llibres i Articles Recomanats</a></li>
<li><a href="07-02-comunitats-forums">Comunitats i Fòrums</a></li>
<li><a href="07-03-eines-llibreries">Eines i Llibreries Útils</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objectiu" rel="nofollow">El Projecte</a> | 
<a href="/sobre-nosaltres" rel="nofollow">Sobre nosaltres</a> | 
<a href="/contribuir" rel="nofollow">Contribuir</a> | 
<a href="/donar" rel="nofollow">Donacions</a> | 
<a href="/llicencia" rel="nofollow">Llicència</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. Tots els drets reservats</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir cookies per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Acceptar</a>
    <a href="/cookies">Més informació</a>
</div>	

		<div class="modal fade" id="loginModal" tabindex="-1" aria-labelledby="loginModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="loginModalLabel">Usuari no autentificat</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
            	<div id="modal-body-main"></div>
            </div>
        </div>
    </div>
</div>	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>
